<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Regression analysis (OLS method)</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #0000ff; } /* Keyword */
code > span.ch { color: #008080; } /* Char */
code > span.st { color: #008080; } /* String */
code > span.co { color: #008000; } /* Comment */
code > span.ot { color: #ff4000; } /* Other */
code > span.al { color: #ff0000; } /* Alert */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #008000; font-weight: bold; } /* Warning */
code > span.cn { } /* Constant */
code > span.sc { color: #008080; } /* SpecialChar */
code > span.vs { color: #008080; } /* VerbatimString */
code > span.ss { color: #008080; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { color: #0000ff; } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #ff4000; } /* Preprocessor */
code > span.do { color: #008000; } /* Documentation */
code > span.an { color: #008000; } /* Annotation */
code > span.cv { color: #008000; } /* CommentVar */
code > span.at { } /* Attribute */
code > span.in { color: #008000; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="libs\style2.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>

<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}

.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Stats in R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="CI.html">CI</a>
</li>
<li>
  <a href="z_t_tests.html">T-test</a>
</li>
<li>
  <a href="F_test.html">F-test</a>
</li>
<li>
  <a href="ChiSquare_test.html">ChiSquare</a>
</li>
<li>
  <a href="regression.html">Regression</a>
</li>
<li>
  <a href="ANOVA.html">ANOVA</a>
</li>
<li>
  <a href="Logistic.html">Logistic</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Regression analysis (OLS method)</h1>

</div>


<p><em>Last modified on 2016-05-21</em></p>
<div id="the-simple-model" class="section level1">
<h1><span class="header-section-number">1</span> The simple model</h1>
<p>The objective of statistical modeling is to come up with the most parsimonious model that does a good job in predicting some variable. The simplest mode is the sample mean. We can express this model as:</p>
<p><span class="math display">\[
Y = \beta_0 + \varepsilon  
\]</span></p>
<p>where <span class="math inline">\(Y\)</span> is the variable we are trying to predict, <span class="math inline">\(\beta_0\)</span> is the mean, and <span class="math inline">\(\varepsilon\)</span> is the difference (or error) between the model, <span class="math inline">\(\beta_0\)</span>, and the actual value <span class="math inline">\(Y\)</span>. How well a model fits the actual data is assessed by measuring the error or deviance between the actual data and the predicted data.</p>
<p>In the following working example, we will look at the median per capita income for the State of Maine aggregated by state (the data are provided from the US Census ACS 2007 - 2011 dataset). The dollar amounts are adjusted to 2010 inflation dollars.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">23663</span>, <span class="dv">20659</span>, <span class="dv">32277</span>, <span class="dv">21595</span>, <span class="dv">27227</span>, 
       <span class="dv">25023</span>, <span class="dv">26504</span>, <span class="dv">28741</span>, <span class="dv">21735</span>, <span class="dv">23366</span>, 
       <span class="dv">20871</span>, <span class="dv">28370</span>, <span class="dv">21105</span>, <span class="dv">22706</span>, <span class="dv">19527</span>, 
       <span class="dv">28321</span>)</code></pre></div>
<p>Next, we will define the simple model (the mean of all 16 values) for this batch of data:</p>
<p><span class="math inline">\(\hat Y = \beta_0 = \bar Y =\)</span> 24481</p>
<p>The “hat” on top of <span class="math inline">\(\hat Y\)</span> indicates that this is a value <em>predicted</em> by the model and not the actual value <span class="math inline">\(Y\)</span>.</p>
<p>We can plot the true values <span class="math inline">\(Y\)</span> (black dots) along with the values predicted from our simple model <span class="math inline">\(\hat Y\)</span> (blue dots and lines).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>( y  , <span class="dt">pch =</span> <span class="dv">16</span>, <span class="dt">ylab=</span><span class="ot">NA</span>, <span class="dt">mgp=</span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">1</span>,.<span class="dv">5</span>) , <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">bty=</span><span class="st">&quot;n&quot;</span>, <span class="dt">xaxs=</span><span class="st">&quot;i&quot;</span>, <span class="dt">xpd =</span> <span class="ot">TRUE</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="kw">mean</span>(y), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)
<span class="kw">points</span>(<span class="kw">rep</span>(<span class="kw">mean</span>(y), <span class="kw">length</span>(y)), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">type=</span><span class="st">&quot;b&quot;</span>, <span class="dt">pch =</span> <span class="dv">21</span>, <span class="dt">bg=</span><span class="st">&quot;white&quot;</span>, <span class="dt">xpd =</span> <span class="ot">TRUE</span>)
<span class="kw">mtext</span>(<span class="st">&quot;Income($)&quot;</span>, <span class="dt">side=</span><span class="dv">3</span>, <span class="dt">adj=</span> -<span class="fl">0.1</span>)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-3-1.png" width="576" /></p>
<p>The function <code>abline</code> adds a blue horizontal line whose value is <span class="math inline">\(\bar Y\)</span>. Blue dots are added using the <code>points()</code> function to highlight the associated predicted values, <span class="math inline">\(\hat Y\)</span>, for each value <span class="math inline">\(Y\)</span>. The x-axis serves only to <em>spread</em> out the <span class="math inline">\(Y\)</span> values across its range. The <code>las = 1</code> option in the <code>plot</code> function sets all axes labels to display horizontally (a value of <code>0</code> would set the orientation of all labels parallel to the axis which is the default setting).</p>
<p>One way to measure the error between the predicted values <span class="math inline">\(\hat Y\)</span> and <span class="math inline">\(Y\)</span> is to compute the difference (error <span class="math inline">\(\varepsilon\)</span>) between both values.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(Y\)</span></th>
<th align="center"><span class="math inline">\(\hat Y\)</span></th>
<th align="center"><span class="math inline">\(\varepsilon = Y - \hat Y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">23663</td>
<td align="center">24481</td>
<td align="center">-818</td>
</tr>
<tr class="even">
<td align="center">20659</td>
<td align="center">24481</td>
<td align="center">-3822</td>
</tr>
<tr class="odd">
<td align="center">32277</td>
<td align="center">24481</td>
<td align="center">7796</td>
</tr>
<tr class="even">
<td align="center">21595</td>
<td align="center">24481</td>
<td align="center">-2886</td>
</tr>
<tr class="odd">
<td align="center">27227</td>
<td align="center">24481</td>
<td align="center">2746</td>
</tr>
<tr class="even">
<td align="center">25023</td>
<td align="center">24481</td>
<td align="center">542</td>
</tr>
<tr class="odd">
<td align="center">26504</td>
<td align="center">24481</td>
<td align="center">2023</td>
</tr>
<tr class="even">
<td align="center">28741</td>
<td align="center">24481</td>
<td align="center">4260</td>
</tr>
<tr class="odd">
<td align="center">21735</td>
<td align="center">24481</td>
<td align="center">-2746</td>
</tr>
<tr class="even">
<td align="center">23366</td>
<td align="center">24481</td>
<td align="center">-1115</td>
</tr>
<tr class="odd">
<td align="center">20871</td>
<td align="center">24481</td>
<td align="center">-3610</td>
</tr>
<tr class="even">
<td align="center">28370</td>
<td align="center">24481</td>
<td align="center">3889</td>
</tr>
<tr class="odd">
<td align="center">21105</td>
<td align="center">24481</td>
<td align="center">-3376</td>
</tr>
<tr class="even">
<td align="center">22706</td>
<td align="center">24481</td>
<td align="center">-1775</td>
</tr>
<tr class="odd">
<td align="center">19527</td>
<td align="center">24481</td>
<td align="center">-4954</td>
</tr>
<tr class="even">
<td align="center">28321</td>
<td align="center">24481</td>
<td align="center">3840</td>
</tr>
</tbody>
</table>
<p>The errors (or differences between <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat Y\)</span>) are displayed as red dashed lines in the following figure:</p>
<p><img src="regression_files/figure-html/unnamed-chunk-5-1.png" width="576" /></p>
<p>To get the magnitude of the total error, we could sum these errors, however, doing so would return a sum of errors of 0 (i.e. the positive errors and negative errors cancel each other out). One solution to circumvent this problem is to take the <strong>absolute</strong> values of the errors then sum these values (this is also known as the <strong>least absolute error</strong>, <strong>LAE</strong>):</p>
<p><span class="math display">\[
LAE = \sum \mid Y - \hat Y \mid
\]</span></p>
<p>However, by convention, we quantify the difference between <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat Y\)</span> using the <strong>sum of the squared errors</strong> (<strong>SSE</strong>) instead, or</p>
<p><span class="math display">\[
SSE = \sum (Y - \hat Y)^2
\]</span></p>
<p>The following table shows the difference between LAE and SSE values</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(Y\)</span></th>
<th align="center"><span class="math inline">\(\hat Y\)</span></th>
<th align="center"><span class="math inline">\(\mid Y - \hat Y \mid\)</span></th>
<th align="center"><span class="math inline">\((Y - \hat Y)^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">23663</td>
<td align="center">24481</td>
<td align="center">818</td>
<td align="center">668511</td>
</tr>
<tr class="even">
<td align="center">20659</td>
<td align="center">24481</td>
<td align="center">3822</td>
<td align="center">14604818</td>
</tr>
<tr class="odd">
<td align="center">32277</td>
<td align="center">24481</td>
<td align="center">7796</td>
<td align="center">60783463</td>
</tr>
<tr class="even">
<td align="center">21595</td>
<td align="center">24481</td>
<td align="center">2886</td>
<td align="center">8326832</td>
</tr>
<tr class="odd">
<td align="center">27227</td>
<td align="center">24481</td>
<td align="center">2746</td>
<td align="center">7542576</td>
</tr>
<tr class="even">
<td align="center">25023</td>
<td align="center">24481</td>
<td align="center">542</td>
<td align="center">294171</td>
</tr>
<tr class="odd">
<td align="center">26504</td>
<td align="center">24481</td>
<td align="center">2023</td>
<td align="center">4094046</td>
</tr>
<tr class="even">
<td align="center">28741</td>
<td align="center">24481</td>
<td align="center">4260</td>
<td align="center">18150795</td>
</tr>
<tr class="odd">
<td align="center">21735</td>
<td align="center">24481</td>
<td align="center">2746</td>
<td align="center">7538457</td>
</tr>
<tr class="even">
<td align="center">23366</td>
<td align="center">24481</td>
<td align="center">1115</td>
<td align="center">1242389</td>
</tr>
<tr class="odd">
<td align="center">20871</td>
<td align="center">24481</td>
<td align="center">3610</td>
<td align="center">13029393</td>
</tr>
<tr class="even">
<td align="center">28370</td>
<td align="center">24481</td>
<td align="center">3889</td>
<td align="center">15127238</td>
</tr>
<tr class="odd">
<td align="center">21105</td>
<td align="center">24481</td>
<td align="center">3376</td>
<td align="center">11394844</td>
</tr>
<tr class="even">
<td align="center">22706</td>
<td align="center">24481</td>
<td align="center">1775</td>
<td align="center">3149294</td>
</tr>
<tr class="odd">
<td align="center">19527</td>
<td align="center">24481</td>
<td align="center">4954</td>
<td align="center">24538401</td>
</tr>
<tr class="even">
<td align="center">28321</td>
<td align="center">24481</td>
<td align="center">3840</td>
<td align="center">14748480</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(LAE_{mean}\)</span> = 50197 and <span class="math inline">\(SSE_{mean}\)</span> = 205233706.</p>
<p>Squaring the error has the effect of <strong>amplifying</strong> large errors in the dataset. The following figure highlights the difference by drawing the <em>square</em> of the differences as red shaded squares.</p>
<p><img src="regression_files/figure-html/unnamed-chunk-7-1.png" width="576" /></p>
<p>It is more natural to visualize a squared amount using a geometric primitive whose area changes proportionately to the squared error changes. Note how the larger error at index 3 has an error line length about 3 times that associated with index 5 (these correspond to the records number three and five in the above table). Yet, the area for index 3 appears almost <em>9 times</em> bigger than that associated with index 5 (as it should since we are <em>squaring</em> the difference). Note too that the unit used to measure the area of each square (the squared error) is the one associated with the <em>Y</em>-axis (recall that the error is associated with the prediction of <span class="math inline">\(Y\)</span> and not of <span class="math inline">\(X\)</span>).</p>
<p>This simple model (i.e. one where we are using the mean value to predict <span class="math inline">\(Y\)</span> for each county) obviously has its limits. A more common approach in predicting <span class="math inline">\(Y\)</span> is to use a <em>predictor</em> variable (i.e. a covariate).</p>
</div>
<div id="the-bivariate-regression-model" class="section level1">
<h1><span class="header-section-number">2</span> The bivariate regression model</h1>
<p>The bivariate model augments the simple (mean) model by adding a variable, <span class="math inline">\(X\)</span>, that is believed to <em>co-vary</em> with <span class="math inline">\(Y\)</span>. In other words, we are using another variable (aka, an <strong>independent</strong> or <strong>predictor</strong> variable) to estimate <span class="math inline">\(Y\)</span>. This augmented model takes on the form:</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1X + \varepsilon
\]</span></p>
<p>This is the equation of a slope where <span class="math inline">\(\beta_0\)</span> tells us where on the y-axis the slope intersects the axis and <span class="math inline">\(\beta_1\)</span> is the slope of the line (a positive <span class="math inline">\(\beta_1\)</span> value indicates that the slope is upward, a negative <span class="math inline">\(\beta_1\)</span> value indicates that the slope is downward).</p>
<p>Continuing with our example, we will look at the percentage of Mainers having attained a Bachelor’s degree or greater by county (the data are provided by the US Census ACS 2007 - 2011 dataset).</p>
<p>The following block of code defines the new variable <span class="math inline">\(X\)</span> (i.e. fraction of Mainers having attained a bachelor’s degree or greater within each county) and visualizes the relationship between income and education attainment using a scatter plot. In the process, a <code>dataframe</code> called <code>dat</code> is created that stores both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># X represents the fraction of residents in each county having attained a</span>
<span class="co"># bachelor&#39;s degree or better</span>
x   &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.19</span>, <span class="fl">0.16</span>, <span class="fl">0.40</span>, <span class="fl">0.24</span>, <span class="fl">0.31</span>, <span class="fl">0.24</span>, <span class="fl">0.28</span>, 
         <span class="fl">0.31</span>, <span class="fl">0.18</span>, <span class="fl">0.23</span>, <span class="fl">0.17</span>, <span class="fl">0.31</span>, <span class="fl">0.15</span>, <span class="fl">0.25</span>, 
         <span class="fl">0.19</span>, <span class="fl">0.28</span>)

<span class="co"># Let&#39;s combine Income and Education into a single dataframe</span>
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Income =</span> y, <span class="dt">Education =</span> x)

<span class="co"># We will add county names to each row</span>
<span class="kw">row.names</span>(dat) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Androscoggin&quot;</span>, <span class="st">&quot;Aroostook&quot;</span>, <span class="st">&quot;Cumberland&quot;</span>, <span class="st">&quot;Franklin&quot;</span>, <span class="st">&quot;Hancock&quot;</span>,
                    <span class="st">&quot;Kennebec&quot;</span>, <span class="st">&quot;Knox&quot;</span>, <span class="st">&quot;Lincoln&quot;</span>, <span class="st">&quot;Oxford&quot;</span>, <span class="st">&quot;Penobscot&quot;</span>, <span class="st">&quot;Piscataquis&quot;</span>,
                    <span class="st">&quot;Sagadahoc&quot;</span>, <span class="st">&quot;Somerset&quot;</span>, <span class="st">&quot;Waldo&quot;</span>, <span class="st">&quot;Washington&quot;</span>, <span class="st">&quot;York&quot;</span>)</code></pre></div>
<p>The following displays the contents of our new dataframe <code>dat</code>:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Income</th>
<th align="right">Education</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Androscoggin</td>
<td align="right">23663</td>
<td align="right">0.19</td>
</tr>
<tr class="even">
<td align="left">Aroostook</td>
<td align="right">20659</td>
<td align="right">0.16</td>
</tr>
<tr class="odd">
<td align="left">Cumberland</td>
<td align="right">32277</td>
<td align="right">0.40</td>
</tr>
<tr class="even">
<td align="left">Franklin</td>
<td align="right">21595</td>
<td align="right">0.24</td>
</tr>
<tr class="odd">
<td align="left">Hancock</td>
<td align="right">27227</td>
<td align="right">0.31</td>
</tr>
<tr class="even">
<td align="left">Kennebec</td>
<td align="right">25023</td>
<td align="right">0.24</td>
</tr>
<tr class="odd">
<td align="left">Knox</td>
<td align="right">26504</td>
<td align="right">0.28</td>
</tr>
<tr class="even">
<td align="left">Lincoln</td>
<td align="right">28741</td>
<td align="right">0.31</td>
</tr>
<tr class="odd">
<td align="left">Oxford</td>
<td align="right">21735</td>
<td align="right">0.18</td>
</tr>
<tr class="even">
<td align="left">Penobscot</td>
<td align="right">23366</td>
<td align="right">0.23</td>
</tr>
<tr class="odd">
<td align="left">Piscataquis</td>
<td align="right">20871</td>
<td align="right">0.17</td>
</tr>
<tr class="even">
<td align="left">Sagadahoc</td>
<td align="right">28370</td>
<td align="right">0.31</td>
</tr>
<tr class="odd">
<td align="left">Somerset</td>
<td align="right">21105</td>
<td align="right">0.15</td>
</tr>
<tr class="even">
<td align="left">Waldo</td>
<td align="right">22706</td>
<td align="right">0.25</td>
</tr>
<tr class="odd">
<td align="left">Washington</td>
<td align="right">19527</td>
<td align="right">0.19</td>
</tr>
<tr class="even">
<td align="left">York</td>
<td align="right">28321</td>
<td align="right">0.28</td>
</tr>
</tbody>
</table>
<p>Now let’s plot the data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>( Income ~<span class="st"> </span>Education , dat, <span class="dt">pch =</span> <span class="dv">16</span>, <span class="dt">ylab=</span><span class="ot">NA</span>, <span class="dt">mgp=</span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">1</span>,.<span class="dv">5</span>) , <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">bty=</span><span class="st">&quot;n&quot;</span>, <span class="dt">xaxs=</span><span class="st">&quot;i&quot;</span>, 
      <span class="dt">xpd =</span> <span class="ot">TRUE</span>, <span class="dt">xlab=</span><span class="st">&quot;Fraction having attained a Bachelor&#39;s degree&quot;</span>)
<span class="kw">mtext</span>(<span class="st">&quot;Income($)&quot;</span>, <span class="dt">side=</span><span class="dv">3</span>, <span class="dt">adj=</span> -<span class="fl">0.1</span>)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-10-1.png" width="576" /></p>
<p>It’s clear from the figure that there appears to be a positive relationship between income and education attainment. The next step is to <em>capture</em> the nature of this relationship from the perspective of a linear equation. We seek to fit a slope that will minimize the overall distance between the line and the true value, hence we seek <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimizes the error. The estimate for <span class="math inline">\(\beta_1\)</span> is sometimes represented as the letter <span class="math inline">\(b_1\)</span> or <span class="math inline">\(\hat \beta_1\)</span>. We’ll stick with the latter notation throughout this exercise. <span class="math inline">\(\hat \beta_1\)</span> is estimated using the linear <strong>least squares</strong> technique: <span class="math display">\[
\hat \beta_1 = \frac{\sum (X_i - \bar X)(Y_i-\bar Y)}{\sum (X_i-\bar X)^2}
\]</span></p>
<p>The breakdown of the terms in this equation are listed in the following table:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right"><span class="math inline">\(X\)</span></th>
<th align="right"><span class="math inline">\(Y\)</span></th>
<th align="right"><span class="math inline">\((X - \bar X)(Y - \bar Y)\)</span></th>
<th align="right"><span class="math inline">\((X - \bar X)^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="right">0.19</td>
<td align="right">23663</td>
<td align="right">43.436</td>
<td align="right">0.003</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="right">0.16</td>
<td align="right">20659</td>
<td align="right">317.673</td>
<td align="right">0.007</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="right">0.40</td>
<td align="right">32277</td>
<td align="right">1223.056</td>
<td align="right">0.025</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="right">0.24</td>
<td align="right">21595</td>
<td align="right">9.018</td>
<td align="right">0.000</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="right">0.31</td>
<td align="right">27227</td>
<td align="right">183.664</td>
<td align="right">0.004</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="right">0.24</td>
<td align="right">25023</td>
<td align="right">-1.695</td>
<td align="right">0.000</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="right">0.28</td>
<td align="right">26504</td>
<td align="right">74.612</td>
<td align="right">0.001</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="right">0.31</td>
<td align="right">28741</td>
<td align="right">284.913</td>
<td align="right">0.004</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="right">0.18</td>
<td align="right">21735</td>
<td align="right">173.318</td>
<td align="right">0.004</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="right">0.23</td>
<td align="right">23366</td>
<td align="right">14.629</td>
<td align="right">0.000</td>
</tr>
<tr class="odd">
<td align="left">11</td>
<td align="right">0.17</td>
<td align="right">20871</td>
<td align="right">263.954</td>
<td align="right">0.005</td>
</tr>
<tr class="even">
<td align="left">12</td>
<td align="right">0.31</td>
<td align="right">28370</td>
<td align="right">260.102</td>
<td align="right">0.004</td>
</tr>
<tr class="odd">
<td align="left">13</td>
<td align="right">0.15</td>
<td align="right">21105</td>
<td align="right">314.355</td>
<td align="right">0.009</td>
</tr>
<tr class="even">
<td align="left">14</td>
<td align="right">0.25</td>
<td align="right">22706</td>
<td align="right">-12.201</td>
<td align="right">0.000</td>
</tr>
<tr class="odd">
<td align="left">15</td>
<td align="right">0.19</td>
<td align="right">19527</td>
<td align="right">263.161</td>
<td align="right">0.003</td>
</tr>
<tr class="even">
<td align="left">16</td>
<td align="right">0.28</td>
<td align="right">28321</td>
<td align="right">141.614</td>
<td align="right">0.001</td>
</tr>
<tr class="odd">
<td align="left">SUM</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">3553.609</td>
<td align="right">0.072</td>
</tr>
</tbody>
</table>
<p>The bottom of the last two columns show the sums of the numerator and denominator (3553.60875 and 0.0715438 respectively). <span class="math inline">\(\hat \beta_1\)</span>, the estimated slope, is therefore 3553.61/0.072 or 49670.</p>
<p>Knowing <span class="math inline">\(\hat \beta_1\)</span> we can solve for the intercept <span class="math inline">\(b_0\)</span>: <span class="math display">\[
\hat \beta_0 = \hat Y - \hat \beta_1 X
\]</span></p>
<p>It just so happens that the slope passes through <span class="math inline">\(\bar Y\)</span> and <span class="math inline">\(\bar X\)</span>. So we can substitute <span class="math inline">\(X\)</span> and <span class="math inline">\(\hat Y\)</span> in the equation with their respective means: 0.24 and 24481. This gives us <span class="math inline">\(\hat \beta_0\)</span> = 12405. Our final linear equation looks like this:</p>
<p><span class="math inline">\(\hat Y\)</span> = 12404.5 + 49670.4 <span class="math inline">\(X\)</span> + <span class="math inline">\(\varepsilon\)</span></p>
<p>It’s no surprise that <code>R</code> has a built in function, <code>lm()</code>, that will estimate these regression coefficients for us. The function can be called as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M &lt;-<span class="st"> </span><span class="kw">lm</span>( y ~<span class="st"> </span>x, dat)</code></pre></div>
<p>The output of the linear model is saved to the variable <code>M</code>. The formula <code>y ~ x</code> tells the function to regress the variable <code>y</code> against the variable <code>x</code>. You must also supply the name of the table or dataframe that stores these variables (i.e. the dataframe <code>dat</code>).</p>
<p>The regression coefficients <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span> computed by <code>lm</code> can be extracted from output <code>M</code> as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M$coefficients</code></pre></div>
<pre><code>(Intercept)           x 
   12404.50    49670.43 </code></pre>
<p>The regression coefficients are identical to those computed manually from the table.</p>
<p>We can also use the model output to add the regression line to our data. The function <code>abline()</code> reads the contents of the model output <code>M</code> and is smart enough to extract the slope equation for plotting:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>( y ~<span class="st"> </span>x , dat, <span class="dt">pch =</span> <span class="dv">16</span>, <span class="dt">ylab=</span><span class="ot">NA</span>, <span class="dt">mgp=</span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">1</span>,.<span class="dv">5</span>) , <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">bty=</span><span class="st">&quot;n&quot;</span>, <span class="dt">xaxs=</span><span class="st">&quot;i&quot;</span>, <span class="dt">xpd =</span> <span class="ot">TRUE</span>,
      <span class="dt">xlab=</span><span class="st">&quot;Fraction having attained a Bachelor&#39;s degree&quot;</span>)
<span class="kw">mtext</span>(<span class="st">&quot;Income($)&quot;</span>, <span class="dt">side=</span><span class="dv">3</span>, <span class="dt">adj=</span> -<span class="fl">0.1</span>)
<span class="kw">abline</span>(M, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-14-1.png" width="576" /></p>
<p>Next, we will look at the errors (or residuals) between the estimated values <span class="math inline">\(\hat Y\)</span> from our model and the true values <span class="math inline">\(Y\)</span> from our observations. We will represent these differences as squared errors:</p>
<p><img src="regression_files/figure-html/unnamed-chunk-15-1.png" width="576" /></p>
<p>Notice how the sizes of the red boxes are smaller then the ones for the simple model (i.e the model defined by the mean <span class="math inline">\(\bar Y\)</span>). In fact, we can sum these squared differences to get our new sum of squared error (SSE) as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SSE &lt;-<span class="st"> </span><span class="kw">sum</span>( <span class="kw">summary</span>(M)$res^<span class="dv">2</span> )</code></pre></div>
<p>This returns an <span class="math inline">\(SSE\)</span> value of 28724435.</p>
<p>This is less than the sum of squared errors for the <em>mean</em> model <span class="math inline">\(\bar Y\)</span>, <span class="math inline">\(SSE_{mean}\)</span>, whose value was 205233705.75. <span class="math inline">\(SSE\)</span> is sometimes referred to as the <strong>residual error</strong>. The difference between the residual error <span class="math inline">\(SSE\)</span> and <span class="math inline">\(SSE_{mean}\)</span> is the error <em>explained</em> by our new (bivariate) model or the <strong>sum of squares reduced</strong>, <span class="math inline">\(SSR\)</span>:</p>
<p><span class="math display">\[
SSR = SSE_{mean} - SSE
\]</span></p>
<p>In <code>R</code>, this can easily be computed as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SSR &lt;-<span class="st"> </span>SSE.mean -<span class="st"> </span>SSE</code></pre></div>
<p>giving us an <span class="math inline">\(SSR\)</span> value of 176509271.</p>
<p>Graphically, the square root of each squared residual making up <span class="math inline">\(SSR\)</span> represents the distance between the <em>mean</em> model (which is a horizontal line) and the <em>bivariate</em> model (which usually has a none zero slope) for every point. This is <em>not</em> the distance between a point and one of the lines.</p>
<p><img src="regression_files/figure-html/unnamed-chunk-18-1.png" width="576" /></p>
<p>The linear models are shown in blue lines, the square root of each residual element making up <span class="math inline">\(SSR\)</span> (the difference between the two models) is shown in dashed red lines. Had we shown each <em>squared</em> residual, red boxes would have been drawn as in earlier figures (the boxes were not drawn here to prevent clutter). The take away point here is that <span class="math inline">\(SSR\)</span> is the error that is explained by the new bivariate model.</p>
<p>The following table summarizes some of these key error terms:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">Error</th>
<th align="center">Notation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mean model</td>
<td align="center">Total error</td>
<td align="center"><span class="math inline">\(SSE_{mean}\)</span> or <span class="math inline">\(SST\)</span></td>
</tr>
<tr class="even">
<td>Bivariate model</td>
<td align="center">Residual error</td>
<td align="center"><span class="math inline">\(SSE\)</span></td>
</tr>
<tr class="odd">
<td>Bivariate - Mean</td>
<td align="center">Explained error</td>
<td align="center"><span class="math inline">\(SSR\)</span></td>
</tr>
</tbody>
</table>
<div id="testing-if-the-bivariate-model-is-an-improvement-over-the-mean-model" class="section level2">
<h2><span class="header-section-number">2.1</span> Testing if the bivariate model is an improvement over the mean model</h2>
<div id="the-coefficient-of-determination-r2" class="section level3">
<h3><span class="header-section-number">2.1.1</span> The coefficient of determination, <span class="math inline">\(R^2\)</span></h3>
<p>In this working example, the percent error explained by the bivariate model is,</p>
<p><span class="math display">\[
\frac{SSE_{mean} - SSE}{SSE_{mean}} \times 100 = \frac{SSR}{SSE_{mean}} \times 100
\]</span></p>
<p>or (205233706 - 28724435) / 205233706 * 100 = <strong>86.0%</strong>. This is a substantial reduction in error.</p>
<p>The ratio between <span class="math inline">\(SSR\)</span> and <span class="math inline">\(SSE_{mean}\)</span> is also referred to as the <strong>proportional error reduction score</strong> (also referred to as the <strong>coefficient of determination</strong>), or <span class="math inline">\(R^2\)</span>, hence:</p>
<p><span class="math display">\[
R^2 = \frac{SSR}{SSE_{mean}}
\]</span></p>
<p><span class="math inline">\(R^2\)</span> is another value computed by the linear model <code>lm()</code> that can be extracted from the output <code>M</code> as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(M)$r.squared</code></pre></div>
<pre><code>[1] 0.8600404</code></pre>
<p>The <span class="math inline">\(R^2\)</span> value computed by <span class="math inline">\(M\)</span> is the same as that computed manually using the ratio of errors (except that the latter was presented as a percentage and not as a fraction). Another way to describe <span class="math inline">\(R^2\)</span> is to view its value as the fraction of the variance in <span class="math inline">\(Y\)</span> explained by <span class="math inline">\(X\)</span>. A <span class="math inline">\(R^2\)</span> value of <span class="math inline">\(0\)</span> implies complete lack of fit of the model to the data whereas a value of <span class="math inline">\(1\)</span> implies perfect fit. In our working example the <span class="math inline">\(R^2\)</span> value of 0.86 implies that the <strong>model explains 86%</strong> of the variance in <span class="math inline">\(Y\)</span>.</p>
<p><code>R</code> also outputs <span class="math inline">\(adjusted\; R^2\)</span>, a better measure of overall model fit. It ‘penalizes’ <span class="math inline">\(R^2\)</span> for the number of predictors in the model <em>vis-a-vis</em> the number of observations. As the ratio of predictors to number of observation increases, <span class="math inline">\(R^2\)</span> can be artificially inflated thus providing us with a false sense of model fit quality. If the number of predictors to number of observations ratio is small, <span class="math inline">\(adjusted\; R^2\)</span> will be smaller than <span class="math inline">\(R^2\)</span>. <span class="math inline">\(adjusted\; R^2\)</span> can be extracted from the model output as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(M)$adj.r.squared</code></pre></div>
<p>The <span class="math inline">\(adjusted\; R^2\)</span> of <strong>0.85</strong> is very close to our <span class="math inline">\(R^2\)</span> value of <strong>0.86</strong>. So our predictive power hasn’t changed given our sample size and one predictor variable.</p>
</div>
<div id="the-f-ratio-test" class="section level3">
<h3><span class="header-section-number">2.1.2</span> The F-ratio test</h3>
<p><span class="math inline">\(R^2\)</span> is one way to evaluate the strength of the linear model. Another is to determine if the reduction in error between the bivariate model and mean model is <em>significant</em>. We’ve already noted a decrease in overall residual errors when augmenting our mean model with a predictor variable <span class="math inline">\(X\)</span> (i.e. the fraction of residents having attained at least a bachelor’s degree). Now we need to determine if this difference is significant.</p>
<p>It helps to conceptualize residual errors as <em>spreads</em> or variances. The following two plots show the errors explained by the model (left) and the errors not explained by the model for each of the 16 data points. Note that we can derive <span class="math inline">\(SSR\)</span> and <span class="math inline">\(SSE\)</span> from the left and right graphs respectively by <em>squaring</em> then <em>summing</em> the horizontal error bars.</p>
<p><img src="regression_files/figure-html/unnamed-chunk-21-1.png" width="576" /></p>
<p>We want to assess whether the total amount of <em>unexplained error</em> (right) is significantly less than the total amount of <em>explained error</em> (left). We have 16 records (i.e. data for sixteen counties) therefore we have 16 measures of error. Since the model assumes that the errors are the same across the entire range of <span class="math inline">\(Y\)</span> values, we will take the average of those errors; hence we take the average of <span class="math inline">\(SSR\)</span> and the average of <span class="math inline">\(SSE\)</span>. However, because these data are from a <em>subset</em> of possible <span class="math inline">\(Y\)</span> values and not <em>all</em> possible <span class="math inline">\(Y\)</span> values we will need to divide <span class="math inline">\(SSR\)</span> and <span class="math inline">\(SSE\)</span> by their respective <span class="math inline">\(degress\; of\; freedom\)</span> and not by the total number of records, <span class="math inline">\(n\)</span>. These “averages” are called <strong>mean squares</strong> (<span class="math inline">\(MS\)</span>) and are computed as follows:</p>
<p><span class="math display">\[
MS_{model} = \frac{SSR}{df_{model}} =  \frac{SSR}{parameters\; in\; the\; bivariate\; model - parameters\; in\; the\; mean\; model}
\]</span> <span class="math display">\[
MS_{residual} = \frac{SSE}{df_{residual}} = \frac{SSE}{n - number\; of\; parameters}
\]</span></p>
<p>where the <span class="math inline">\(parameters\)</span> are the number of coefficients in the mean model (where there is just one parameter: <span class="math inline">\(\beta_0 = \hat Y\)</span>) and the number of coefficient in the bivariate model (where there are two parameters: <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>). In our working example, <span class="math inline">\(df_{model}\)</span> = 1 and <span class="math inline">\(df_{residual}\)</span> = 14.</p>
<p>It’s important to remember that these measures of error are measures of <strong>spread</strong> and not of a <em>central</em> value. Since these are measures of spread we use the <span class="math inline">\(F\)</span>-test (and <em>not</em> the <span class="math inline">\(t\)</span>-test) to determine if the difference between <span class="math inline">\(MS_{model}\)</span> and <span class="math inline">\(MS_{residual}\)</span> is significant (recall that the <span class="math inline">\(F\)</span> test is used to determine if two measures of spread between samples are significantly different). The <span class="math inline">\(F\)</span> ratio is computed as follows:</p>
<p><span class="math display">\[
F = \frac{MS_{model}}{MS_{residual}}
\]</span></p>
<p>A simple way to implement an <span class="math inline">\(F\)</span> test with on our bivariate regression model is to call the <code>anova()</code> function as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(M)</code></pre></div>
<p>where the variable <code>M</code> is the output from our regression model created earlier. The output of <code>anova()</code> is summarized in the following table:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Df</th>
<th align="center">Sum Sq</th>
<th align="center">Mean Sq</th>
<th align="center">F value</th>
<th align="center">Pr(&gt;F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">x</td>
<td align="center">1</td>
<td align="center">176509271</td>
<td align="center">176509271</td>
<td align="center">86.02884</td>
<td align="center">0.0000002</td>
</tr>
<tr class="even">
<td align="left">Residuals</td>
<td align="center">14</td>
<td align="center">28724435</td>
<td align="center">2051745</td>
<td align="center">NA</td>
<td align="center">NA</td>
</tr>
</tbody>
</table>
<p>The first row in the table (labeled as <code>x</code>) represents the error terms <span class="math inline">\(SSR\)</span> and <span class="math inline">\(MS_{model}\)</span> for the basic model (the <em>mean</em> model in our case) while the second row labeled <code>Residuals</code> represents error terms <span class="math inline">\(SSE\)</span> and <span class="math inline">\(MS_{residual}\)</span> for the current model. The column labeled <code>Df</code> displays the degrees of freedom, the column labeled <code>Sum Sq</code> displays the sum of squares <span class="math inline">\(SSR\)</span> and <span class="math inline">\(SSE\)</span>, the column labeled <code>Mean Sq</code> displays the <em>mean squares</em> <span class="math inline">\(MS\)</span> and the column labeled <code>F value</code> displays the <span class="math inline">\(F\)</span> ratio <span class="math inline">\(MS_{model}/MS_{residual}\)</span>.</p>
<p>The larger the <span class="math inline">\(F\)</span> ratio, the greater the difference between the bivariate model and mean model. The question then becomes ‘how significant is the observed ratio?’. To answer this question, we must setup a hypothesis test. We setup the test as follows:</p>
<p><span class="math inline">\(H_o\)</span>: The addition of the term <span class="math inline">\(\beta_1\)</span> (and hence the predictor <span class="math inline">\(X\)</span>) does not improve the prediction of <span class="math inline">\(Y\)</span> over the simple <em>mean</em> model.</p>
<p><span class="math inline">\(H_a\)</span>: The addition of the term <span class="math inline">\(\beta_1\)</span> (and hence the predictor <span class="math inline">\(X\)</span>) helps improve the prediction of <span class="math inline">\(Y\)</span> over the simple <em>mean</em> model.</p>
<p>The next step is to determine how likely we are (as a measure of probability) to have a <span class="math inline">\(F\)</span> ratio as large as the one observed under the assumption <span class="math inline">\(H_o\)</span> that the bivariate model does <em>not</em> improve on our prediction of <span class="math inline">\(Y\)</span>. The last column in the <code>anova</code> output gives us this probability (as a fraction): <span class="math inline">\(P\)</span> = 0. In other words, there is a 0% chance that we could have computed an <span class="math inline">\(F\)</span> ratio as extreme as ours had the <em>bivariate</em> model not improved over the <em>mean</em> model.</p>
<p>The following graph shows the frequency distribution of <span class="math inline">\(F\)</span> values we would expect if indeed <span class="math inline">\(H_o\)</span> was true. Most of the values under <span class="math inline">\(H_o\)</span> fall between 0 and 5. The red line to the far right of the curve is where our observed <span class="math inline">\(F\)</span> value lies along this continuum–this is not a value one would expect to get if <span class="math inline">\(H_o\)</span> were true. It’s clear from this output that our <em>bivariate</em> model is a significant improvement over our <em>mean</em> model.</p>
<p><img src="regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>The F-ratio is not only used to compare the bivariate model to the mean, in fact, it’s more commonly used to compare a <em>two-predictor</em> variable model with a <em>one-predictor</em> variable model or a <em>three-predictor</em> model with a <em>two-predictor</em> model and so on. In each case, one model is compared with a similar model <em>minus</em> one predictor variable.</p>
<p>When two different models are compared, the function <code>anova()</code> requires two parameters: the two regression model outputs (we’ll call <code>M1</code> and <code>M2</code>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(M1, M2)</code></pre></div>
<p>An example will be provided in a later section when we tackle multivariate regression.</p>
</div>
</div>
<div id="testing-if-the-estimated-slope-is-significantly-different-from-0" class="section level2">
<h2><span class="header-section-number">2.2</span> Testing if the estimated slope is significantly different from 0</h2>
<p>We’ve just assessed that our <em>bivariate</em> model is a significant improvement over the much <em>simpler</em> mean model. We can also test if each regression coefficient <span class="math inline">\(\beta\)</span> is significantly different from 0. If we are dealing with a model that has just one predictor <span class="math inline">\(X\)</span>, then the <span class="math inline">\(F\)</span> test just described will also tell us if the regression coefficient <span class="math inline">\(\beta_1\)</span> is significant. However, if more than one predictor variable is present in the regression model, then you should perform an <span class="math inline">\(F\)</span>-test to test <em>overall</em> model improvement, then test each regression term independently for significance.</p>
<p>When assessing if a regression coefficient is significantly different from zero, we are setting up yet another hypothesis test where:</p>
<p><span class="math inline">\(H_o\)</span>: <span class="math inline">\(\beta_1\)</span> = 0 (i.e. the predictor variable <em>does not</em> contribute significantly to the overall improvement of the predictive ability of the model).</p>
<p><span class="math inline">\(H_a\)</span>: <span class="math inline">\(\beta_1\)</span> &lt; 0 or <span class="math inline">\(\beta_1\)</span> &gt; 0 for one-tailed test, or <span class="math inline">\(\beta_1 \neq\)</span> 0 for a two-tailed test (i.e. the predictor variable <em>does</em> contribute significantly to the overall improvement of the predictive ability of the model).</p>
<p>Our hypothesis in this working example is that the level of education attainment (<span class="math inline">\(X\)</span>) can predict per-capita income (<span class="math inline">\(Y\)</span>), at least at the county level (which is the level at which our data is aggregated). So what we set out to test is the <strong>null hypothesis</strong>, <span class="math inline">\(H_o\)</span>, that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>not linearly related</strong>. If <span class="math inline">\(H_o\)</span> is true, then we would expect the regression coefficient <span class="math inline">\(\beta_1\)</span> to be very close to zero. So this begs the question ‘how close should <span class="math inline">\(\beta_1\)</span> be to 0 for us <strong>not to reject</strong> <span class="math inline">\(H_o\)</span>?’</p>
<p>To answer this question, we need to perform a <span class="math inline">\(t\)</span>-test where we compare our calculated regression coefficient <span class="math inline">\(\hat \beta_1\)</span> to what we would expect to get if <span class="math inline">\(X\)</span> did not contribute significantly to the model’s predictive capabilities. So the statistic <span class="math inline">\(t\)</span> is computed as follows:</p>
<p><span class="math display">\[
t = \frac{\hat \beta_1 - \beta_{1_o}}{s_{\hat \beta_1}}
\]</span></p>
<p>Since <span class="math inline">\(\beta_1\)</span> is zero under <span class="math inline">\(H_o\)</span> , we can rewrite the above equation as follows: <span class="math display">\[
t = \frac{\hat \beta_1 - 0}{s_{\hat \beta_1}} = \frac{\hat \beta_1 }{s_{\hat \beta_1}}
\]</span></p>
<p>where the estimate <span class="math inline">\(s_{\hat \beta_1}\)</span> is the <strong>standard error of <span class="math inline">\(\beta\)</span></strong> which can be computed from:</p>
<p><span class="math display">\[
s_{\hat \beta_1} = \frac{\sqrt{MS_{residual}}}{\sqrt{SSX}}
\]</span></p>
<p>where <span class="math inline">\(SSX\)</span> is the sum of squares of the variable <span class="math inline">\(X\)</span> (i.e. <span class="math inline">\(\sum (X - \bar X)^2\)</span>) which was computed in an earlier table.</p>
<p>In our working example, <span class="math inline">\(s_{\hat \beta_1}\)</span> is 1432.3914787 / 0.2674766 or <span class="math inline">\(s_{\hat \beta_1}\)</span> = 5355.2. The test statistic <span class="math inline">\(t\)</span> is therefore 49670.43 / 5355.2 or <span class="math inline">\(t\)</span> = 9.28.</p>
<p>Next we need to determine if the computed value of <span class="math inline">\(t\)</span> is significantly different from the values of <span class="math inline">\(t\)</span> expected under <span class="math inline">\(H_o\)</span>. The following figure plots the frequency distribution of <span class="math inline">\(\beta_{1_o}\)</span> (i.e. the kind of <span class="math inline">\(\beta_1\)</span> values we could expect to get under the assumption that the predictor <span class="math inline">\(X\)</span> does not contribute to the model) along with the red regions showing where our observed <span class="math inline">\(\hat \beta_1\)</span> must lie for us to safely reject <span class="math inline">\(H_o\)</span> at the 5% confidence level (note that because we are performing a two-sided test, i.e. that <span class="math inline">\(\hat \beta_1\)</span> is different from 0, the red regions each represent 2.5%; when combined both tails give us a 5% rejection region).</p>
<p><img src="regression_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>It’s clear that seeing where our observed <span class="math inline">\(\hat \beta_1\)</span> lies along the distribution allows us to be fairly confident that our <span class="math inline">\(\hat \beta_1\)</span> is far from being a <em>typical value</em> under <span class="math inline">\(H_o\)</span>, in fact we can be quite confident in rejecting the null hypothesis, <span class="math inline">\(H_o\)</span>. This implies that <span class="math inline">\(X\)</span> (education attainment) <strong>does</strong> contribute information towards the prediction of <span class="math inline">\(Y\)</span> (per-capita income) when modeled as a <em>linear</em> relationship.</p>
<p>Fortunately, we do not need to burden ourselves with all these calculations. The aforementioned standard error, <span class="math inline">\(t\)</span> value and <span class="math inline">\(P\)</span> value are computed as part of the <code>lm</code> analysis. You can list these and many other output parameters using the <code>summary()</code> function as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(M)[[<span class="dv">4</span>]]</code></pre></div>
<p>The <code>[[4]]</code> option displays the pertinent subset of the <code>summary</code> output that we want. The content of this output is displayed in the following table:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="center">12404.50</td>
<td align="center">1350.332</td>
<td align="center">9.186264</td>
<td align="center">0.0000003</td>
</tr>
<tr class="even">
<td align="left">x</td>
<td align="center">49670.43</td>
<td align="center">5355.202</td>
<td align="center">9.275173</td>
<td align="center">0.0000002</td>
</tr>
</tbody>
</table>
<p>The second line of the output displays the value of <span class="math inline">\(\hat \beta_1\)</span>, its standard error, its <span class="math inline">\(t\)</span>-value and the probability of observing a <span class="math inline">\(\beta_1\)</span> value as extreme as ours under <span class="math inline">\(H_o\)</span>.</p>
<div id="extracting-hat-beta_1-confidence-interval" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Extracting <span class="math inline">\(\hat \beta_1\)</span> confidence interval</h3>
<p><span class="math inline">\(\hat \beta_1\)</span>’s standard error <span class="math inline">\(s_{\hat \beta_1}\)</span> can also be used to derive a confidence interval for our estimate. The standard error of <strong>5355</strong> tells us that we can be ~68% confident that our true <span class="math inline">\(\beta_1\)</span> value lies between <span class="math inline">\(\hat \beta_1\)</span> - 5355 and <span class="math inline">\(\hat \beta_1\)</span> + 5355. We can use the standard error to construct different confidence intervals depending on our desired <span class="math inline">\(\alpha\)</span> level. The following table shows upper and lower limits for a few common alpha levels:</p>
<table>
<thead>
<tr class="header">
<th align="left">Confidence level <span class="math inline">\(\alpha\)</span></th>
<th align="center">Lower limit</th>
<th align="center">Upper limit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">68%</td>
<td align="center">44149</td>
<td align="center">55192</td>
</tr>
<tr class="even">
<td align="left">95%</td>
<td align="center">38185</td>
<td align="center">61156</td>
</tr>
<tr class="odd">
<td align="left">99%</td>
<td align="center">33729</td>
<td align="center">65612</td>
</tr>
</tbody>
</table>
<p>The following figure shows the range of the distribution curve covered by the 68% and 95% confidence intervals for our working example.</p>
<p><img src="regression_files/figure-html/unnamed-chunk-30-1.png" width="768" /></p>
<p>The confidence intervals for <span class="math inline">\(\hat \beta_1\)</span> can easily be extracted in <code>R</code> using the <code>confint()</code> function. for example, to get the confidence interval for an <span class="math inline">\(\alpha\)</span> of 95%, call the following function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(M, <span class="dt">parm =</span> <span class="dv">2</span>, <span class="dt">level =</span> <span class="fl">0.95</span>)</code></pre></div>
<pre><code>     2.5 %  97.5 %
x 38184.66 61156.2</code></pre>
<p>The first parameter, <code>M</code>, is the linear regression model output computed earlier. The second parameter <code>parm = 2</code> tells <code>R</code> which parameter to compute the confidence interval for (a value of <code>1</code> means that the confidence interval for the intercept is desired), and the third parameter in the function, <code>level = 0.95</code> tells <code>R</code> which <span class="math inline">\(\alpha\)</span> value you wish to compute a confidence interval for.</p>
</div>
</div>
<div id="checking-the-residuals" class="section level2">
<h2><span class="header-section-number">2.3</span> Checking the residuals</h2>
<p>We have already used the residuals <span class="math inline">\(\varepsilon\)</span> to give us and assessment of model fit and the contribution of <span class="math inline">\(X\)</span> to the prediction of <span class="math inline">\(Y\)</span>. But the <em>distribution</em> of the residuals can also offer us insight into whether or not all of the following key assumptions are met:</p>
<ol style="list-style-type: decimal">
<li>The <strong>mean</strong> of the residuals, <span class="math inline">\(\bar \varepsilon = 0\)</span>, is close to <strong>0</strong>.</li>
<li>The <strong>spread</strong> of the residuals is the same for all values of <span class="math inline">\(X\)</span>–i.e. they should be <strong>homoscedastic</strong>.</li>
<li>The probability distribution of the errors follows a <strong>normal (Gaussian)</strong> distribution.</li>
<li>The residuals are <strong>independent</strong> of one another–i.e. they are not <strong>autocorrelated</strong>.</li>
</ol>
<p>We will explore each assumption in the following sections.</p>
<div id="assumption-1-bar-varepsilon-0" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Assumption 1: <span class="math inline">\(\bar \varepsilon = 0\)</span></h3>
<p>The <code>lm</code> regression model outputs not only parameters as seen in earlier sections, but residuals as well. For each <span class="math inline">\(Y\)</span> value, a residual (or error) accounting for the difference between the model <span class="math inline">\(\hat Y\)</span> and actual <span class="math inline">\(Y\)</span> value is computed. We can plot the residuals as a function of the predictor variable <span class="math inline">\(X\)</span>. We will also draw the line at <span class="math inline">\(\varepsilon\)</span> = 0 to see how <span class="math inline">\(\varepsilon\)</span> fluctuates around 0.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(M$residuals ~<span class="st"> </span>x, <span class="dt">pch =</span> <span class="dv">16</span>, <span class="dt">ylab=</span> <span class="st">&quot;Residuals&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h =</span> <span class="dv">0</span>, <span class="dt">lty =</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-32-1.png" width="384" /></p>
<p>We can compute the mean of the residuals, <span class="math inline">\(\bar \varepsilon\)</span>, as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(M$residuals)</code></pre></div>
<pre><code>[1] -0.00000000000002131628</code></pre>
<p>which gives us a value very close to zero as expected.</p>
</div>
<div id="assumption-2-homoscedasticity" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Assumption 2: homoscedasticity</h3>
<p>It helps to demonstrate via figures what does and does not constitute a homoscedastic residual:</p>
<p><img src="regression_files/figure-html/unnamed-chunk-34-1.png" width="864" /></p>
<p>The figure on the left shows no obvious trend in the <em>variability</em> of the residuals as a function of <span class="math inline">\(X\)</span>. The middle figure shows an <em>increase</em> in variability of the residuals with increasing <span class="math inline">\(X\)</span>. The figure on the right shows a <em>decrease</em> in variability in the residuals with increasing <span class="math inline">\(X\)</span>. When the variances of <span class="math inline">\(\varepsilon\)</span> are not uniform across all ranges of <span class="math inline">\(X\)</span>, the residuals are said to be <strong>heteroscedastic</strong>.</p>
<p>Our plot of the residuals from the bivariate model may show some signs of heteroscedasticity, particularly on the right hand side of the graph. When dealing with relatively small samples, a visual assessment of the distribution of the residuals may not be enough to assess whether or not the assumption of homoscedasticity is met. We therefore turn to the <strong>Breusch-Pagan test</strong>. A Breusch-Pagan function, <code>ncvTest()</code>, is available in the library <code>car</code> which will need to be loaded into our <code>R</code> session before performing the test. The package <code>car</code> is usually installed as part of the normal <code>R</code> installation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(car)
<span class="kw">ncvTest</span>(M)</code></pre></div>
<pre><code>Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.2935096    Df = 1     p = 0.5879802 </code></pre>
<p>The Breusch-Pagan tests the (null) hypothesis that the variances are constant across the full range of <span class="math inline">\(X\)</span>. The <span class="math inline">\(P\)</span> value tells the likelihood that our distribution of residual variances are consistent with homoscedasticity. A non-significant <span class="math inline">\(P\)</span> value (one usually greater than 0.05) should indicate that our residuals are homoscedastic. Given our high <span class="math inline">\(P\)</span> value of 0.59, we can be fairly confident that we have satisfied our second assumption.</p>
</div>
<div id="assumption-3-normal-gaussian-distribution-of-residuals" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Assumption 3: Normal (Gaussian) distribution of residuals</h3>
<p>An important assumption is that the distribution of the residual values follow a normal (Gaussian) distribution. Note that this assumption is <strong>only</strong> needed when computing <strong>confidence intervals</strong> or regression coefficient <strong><span class="math inline">\(P\)</span>-values</strong>. If your interest is solely in finding the best linear <em>unbiased</em> estimator (BLUE), then only the three other assumptions need to be met.</p>
<p>The assumption of normality of residuals is often confused with the belief that <span class="math inline">\(X\)</span> (the predictor variables) must follow a normal distribution. This is incorrect! The least-squares regression model makes <strong>no</strong> assumptions about the distribution of <span class="math inline">\(X\)</span>. However, if the <em>residuals</em> do not follow a normal distribution, then one method of resolving this problem is to transform the values of <span class="math inline">\(X\)</span> (which may be the source of confusion).</p>
<p>It helps to first plot the histogram of our residuals then to fit a kernel density function to help see the overall trend in distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(M$residuals, <span class="dt">col=</span><span class="st">&quot;bisque&quot;</span>, <span class="dt">freq=</span><span class="ot">FALSE</span>, <span class="dt">main=</span><span class="ot">NA</span>)
<span class="kw">lines</span>(<span class="kw">density</span>(M$residuals), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-36-1.png" width="384" /></p>
<p>A good visual test of normality is the use of a <strong>Q-Q plot</strong>. We will once again turn to the <code>car</code> package and use its <code>qqPlot()</code> function (be careful not to confuse this <code>qqPlot</code> function with <code>R</code>‘s standard <code>qqplot</code> which uses a <em>lowercase</em> ’p’).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(car) <span class="co"># Load this package if not already loaded</span>
<span class="kw">qqPlot</span>(M, <span class="dt">lw=</span><span class="dv">1</span>)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-37-1.png" width="384" /></p>
<p>What we are hoping to see is an alignment of the residual values (hollow points on the plot) along the red sloped line. Any deviation from the red sloped line may indicates lack of normality in our residuals. You will seldom encounter residuals that fit the line exactly. What you are assessing is whether or not the difference is significant. In our working example, there seems to be disagreement between the distribution of our residuals and the <em>normal</em> line on the plot.</p>
<p>A visual assessment of normality using the Q-Q plot is usually the preferred approach, but you can also use the <strong>Shapiro-Wilk</strong> test to assess the <em>significance</em> of deviation from normality. But use this test with caution, large sample sizes tend to always indicate deviation from normality, regardless how close they follow the Gaussian curve. <code>R</code> has a function called <code>shapiro.test()</code> which we can call as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">shapiro.test</span>(M$residuals)</code></pre></div>
<pre><code>
    Shapiro-Wilk normality test

data:  M$residuals
W = 0.92243, p-value = 0.1846</code></pre>
<p>The output we are interested in is the <span class="math inline">\(P\)</span>-value. The <em>Shapiro-Wilk Test</em> tests the null hypothesis that the residuals come from a normally distributed population. A large <span class="math inline">\(P\)</span>-value indicates that there is a good chance that the null is true (i.e. that the residuals are close to being normally distributed). If the <span class="math inline">\(P\)</span> value is small, then there is a good chance that the null is false. Our <span class="math inline">\(P\)</span> value is 0.18 which would indicate that we cannot reject the null. Despite the Shapiro-Welk test telling us that there is a good chance that our <span class="math inline">\(\varepsilon\)</span> values come from a normally distributed population, our visual assessment of the Q-Q plot leads us to question this assessment.</p>
<p>Again, remember that the assumption of normality of residuals really only matters when you are seeking a confidence interval for <span class="math inline">\(\hat \beta_1\)</span> or <span class="math inline">\(P\)</span>-values.</p>
</div>
<div id="assumption-4-independence-of-residuals" class="section level3">
<h3><span class="header-section-number">2.3.4</span> Assumption 4: Independence of residuals</h3>
<p>Our final assumption pertains to the independence of the residuals. In other words, we want to make sure that a residual value is not auto-correlated with a <em>neighboring</em> residual value. The word <em>neighbor</em> can mean different things. It may refer to the <em>order</em> of the residuals in which case we are concerned with residuals being more similar within a narrow range of <span class="math inline">\(X\)</span>, or it may refer to a <em>spatial</em> neighborhood in which case we are concerned with residuals of similar values being <em>spatially</em> clustered. To deal with the former, we can use the <strong>Durbin-Watson</strong> test available in the <code>R</code> package <code>car</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(car) <span class="co"># Load this package if not already loaded</span>
<span class="kw">durbinWatsonTest</span>(M)</code></pre></div>
<pre><code> lag Autocorrelation D-W Statistic p-value
   1      0.01856517      1.706932   0.638
 Alternative hypothesis: rho != 0</code></pre>
<p>Ideally, the <code>D-W</code> statistic returned by the test should fall within the range of 1 to 3. The <span class="math inline">\(P\)</span>-value is the probability that our residual distribution is consistent with what we would expect if there was no auto-correlation. Our test statistic of 1.71 and <span class="math inline">\(P\)</span> value of 0.65 suggests that the assumption of independence is met with our model. You might note that the <span class="math inline">\(P\)</span>-value changes every time the tests is re-run. This is because the Durbin Watson test, as implemented in <code>R</code>, uses a Monte-Carlo approach to compute <span class="math inline">\(P\)</span>. If you want to nail <span class="math inline">\(P\)</span> down to a greater precision, you can add the <code>reps =</code> parameter to the function (by default, the test runs 1000 bootstrap replications). For example, you could rerun the test using 10,000 bootstrap replications (<code>durbinWatsonTest(M, reps = 10000)</code>).</p>
</div>
</div>
<div id="influential-observations" class="section level2">
<h2><span class="header-section-number">2.4</span> Influential Observations</h2>
<p>We want to avoid a situation where a single, or very small subset of points, have a disproportionately large influence on the model results. It is usually best to remove such influential points from the regression analysis. This is not to say that the influential points should be ignored from the overall analysis, but it may suggest that such points may behave differently then the bulk of the data and therefore may require a different model.</p>
<p>Several tests are available to determine if one or several points are influential. Two of which are covered here: <strong>Cook’s distance</strong> and <strong>hat values</strong>.</p>
<p>Cook’s distance can be computed from the model using the <code>cooks.distance()</code> function. But it’s also available as one of <code>plot.lm</code>’s diagnostic outputs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>( M, <span class="dt">which =</span> <span class="dv">4</span>)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-40-1.png" width="384" /></p>
<p>We could have called the <code>plot.lm</code> function, but because <code>R</code> recognizes that the object <code>M</code> is the output of an <code>lm</code> regression, it automatically passes the call to <code>plot.lm</code>. The option <code>which = 4</code> tells <code>R</code> which of the 6 diagnostic plots to display (to see a list of all diagnostic plots offered, type <code>?plot.lm</code>). Usually, any point with a Cook’s distance value <strong>greater than 1</strong> is considered overly influential. In our working example, none of the points are even close to 1 implying that none of our observations wield undue influence.</p>
<p>The other test that can be used to assess if a point has strong leverage is the <strong>hat values</strong> test. The technique involves calculating an average leverage value for all data points; this is simply the ratio between the number of regression coefficients in our model (this includes the intercept) and the number of observations. Once the average leverage is computed, we use a cutoff of either twice this average or three times this average (these are two popular cutoff values). We then look for hat values greater then these cutoffs. In the following chunk of code, we first compute the mean leverage values (whose value is assigned to the <code>cut</code> object), we then compute the leverage values for each point using the <code>hatvalues()</code> function and plot the resulting leverage values along with the two cutoff values (in dashed red lines).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cut   &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>) *<span class="st"> </span><span class="kw">length</span>(<span class="kw">coefficients</span>(M)) /<span class="st"> </span><span class="kw">length</span>(<span class="kw">resid</span>(M))
M.hat &lt;-<span class="st"> </span><span class="kw">hatvalues</span>(M)
<span class="kw">plot</span>(M.hat)
<span class="kw">abline</span>(<span class="dt">h =</span> cut, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">text</span>( <span class="kw">which</span>(M.hat &gt;<span class="st"> </span><span class="kw">min</span>(cut) ) ,  M.hat[M.hat &gt;<span class="st"> </span><span class="kw">min</span>(cut)] , 
      <span class="kw">row.names</span>(dat)[M.hat &gt;<span class="st"> </span><span class="kw">min</span>(cut)], <span class="dt">pos=</span><span class="dv">4</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-41-1.png" width="384" /></p>
<p>The last line of code (the one featuring the <code>text</code> function) labels only the points having a value greater than the smaller of the two cutoff values.</p>
<p>The influential point of interest is associated with the third record in our <code>dat</code> dataset, Cumberland county. Let’s see where the point lies relative to the regression slope</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(Income ~<span class="st"> </span>Education, dat)
<span class="kw">abline</span>(M, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)
<span class="kw">points</span>(dat[M.hat &gt;<span class="st"> </span><span class="kw">min</span>(cut),]$Education, 
       dat[M.hat &gt;<span class="st"> </span><span class="kw">min</span>(cut),]$Income, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>) 
<span class="kw">text</span>( dat[M.hat &gt;<span class="st"> </span><span class="kw">min</span>(cut),]$Education, 
       dat[M.hat &gt;<span class="st"> </span><span class="kw">min</span>(cut),]$Income , 
       <span class="dt">labels =</span> <span class="kw">row.names</span>(dat)[M.hat &gt;<span class="st"> </span><span class="kw">min</span>(cut)], 
      <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">pos=</span><span class="dv">2</span>) <span class="co"># Add label to the left of point</span></code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-42-1.png" width="384" /></p>
<p>The point lies right on the line and happens to be at the far end of the distribution. The concern is that this point might have undue leverage potential. It helps to think of the regression line as a long straight bar hinged somewhere near the center. It requires less <em>force</em> to move the bar about the imaginary hinge point when applied to the ends of the bar then near the center of the bar. The <em>hat values</em> test is suggesting that observation number 3 may have unusual leverage potential. We can assess observation number 3’s influence by running a new regression analysis without observation number 3.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a new dataframe that omits observation number 3</span>
dat.nolev &lt;-<span class="st"> </span>dat[ -<span class="st"> </span><span class="kw">which</span>(M.hat &gt;<span class="st"> </span><span class="kw">min</span>(cut)),]

<span class="co"># Run a new regression analysis</span>
M.nolev &lt;-<span class="st"> </span><span class="kw">lm</span>( Income ~<span class="st"> </span>Education, dat.nolev)</code></pre></div>
<p>We can view a full summary of this analysis using the <code>summary()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(M.nolev)</code></pre></div>
<pre><code>
Call:
lm(formula = Income ~ Education, data = dat.nolev)

Residuals:
    Min      1Q  Median      3Q     Max 
-2730.0  -518.1   306.4   819.1  2009.8 

Coefficients:
            Estimate Std. Error t value   Pr(&gt;|t|)    
(Intercept)    12408       1670   7.431 0.00000497 ***
Education      49654       6984   7.109 0.00000794 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1486 on 13 degrees of freedom
Multiple R-squared:  0.7954,    Adjusted R-squared:  0.7797 
F-statistic: 50.54 on 1 and 13 DF,  p-value: 0.000007942</code></pre>
<p>You’ll note that the <span class="math inline">\(adjusted\; R^2\)</span> value drops from 0.85 to 0.78.</p>
<p>If we compare the estimates (and their standard errors) between both model (see the summary tables below), we’ll note that the estimates are nearly identical, however, the standard errors for <span class="math inline">\(\hat \beta_1\)</span> increase by almost 30%.</p>
<p>The original model <code>M</code>:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="center">12404.50</td>
<td align="center">1350.33</td>
<td align="center">9.19</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="left">x</td>
<td align="center">49670.43</td>
<td align="center">5355.20</td>
<td align="center">9.28</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>The new model <code>M.nolev</code>:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="center">12407.93</td>
<td align="center">1669.77</td>
<td align="center">7.43</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="left">Education</td>
<td align="center">49654.45</td>
<td align="center">6984.52</td>
<td align="center">7.11</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>Given that our estimates are nearly identical, but that the overall confidence in the model decreases with the new model, there seems to be no reason why we would omit the 3rd observation based on this analysis.</p>
<p>This little exercise demonstrates the need to use as many different tools as possible to evaluate whether an assumption is satisfied or not.</p>
</div>
<div id="what-to-do-if-some-of-the-assumptions-are-not-met" class="section level2">
<h2><span class="header-section-number">2.5</span> What to do if some of the assumptions are not met</h2>
<div id="data-transformation" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Data transformation</h3>
<p>Data transformation (usually via some non-linear re-expression) may be required when one or more of the following apply: * The residuals are skewed or are heteroscedastic. * When theory suggests. * To force a linear relationship between variables</p>
<p><strong>Do not</strong> transform your data if the sole purpose is to “correct” for outliers.</p>
<p>We observed that our residuals did not follow a normal (Gaussian) distribution. Satisfying this assumption is important if we are to use this model to derive confidence intervals around our <span class="math inline">\(\beta\)</span> estimates or if we are to use this model to compute <span class="math inline">\(P\)</span> values. We can see if transforming our data will help. This means re-expressing the <span class="math inline">\(X\)</span> and/or the <span class="math inline">\(Y\)</span> values (i.e. converting each value in <span class="math inline">\(X\)</span> and/or <span class="math inline">\(Y\)</span> by some expression such as <code>log(x)</code> or <code>log(y)</code>)</p>
<p>It helps to look at the distribution of the variables (this is something that should normally be done prior to performing a regression analysis). We’ll use the <code>hist()</code> plotting function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(dat$Income, <span class="dt">breaks =</span> <span class="kw">with</span>(dat, <span class="kw">seq</span>(<span class="kw">min</span>(Income), <span class="kw">max</span>(Income), <span class="dt">length.out=</span><span class="dv">5</span>)) )
<span class="kw">hist</span>(dat$Education, <span class="dt">breaks =</span> <span class="kw">with</span>(dat, <span class="kw">seq</span>(<span class="kw">min</span>(Education), <span class="kw">max</span>(Education), <span class="dt">length.out=</span><span class="dv">5</span>)) )</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-48-1.png" width="576" /></p>
<p>Two common transformations are the natural log (implemented as <code>log</code> in <code>R</code>) and the square. These are special cases of what is called a <strong>Box-Cox</strong> family of transformations.</p>
<p>Data transformation is an iterative process which relies heavily on <strong>Exploratory Data Analysis (EDA)</strong> skills. It’s important to keep in mind the goal of the transformation. In our example, its purpose is to help improve the symmetry of the residuals. The following figures show different Q-Q plots of the regression residuals for different transformations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Several data transformation scenarios were explored using square root and logarithmic transformation of the <span class="math inline">\(X\)</span>’s and/or <span class="math inline">\(Y\)</span>’s. Of course, a thorough analysis would consist of a wide range of Box-Cox transformations. The scatter plots (along with the regression lines) are displayed on the left hand side for different transformations (the axes labels indicate which, if any, transformation was applied). The accompanying Q-Q plots are displayed to the right of the scatter plots.</p>
<p><img src="regression_files/figure-html/unnamed-chunk-49-1.png" width="624" /></p>
<p>A visual assessment of the Q-Q plots indicates some mild improvements near the middle of the residual distribution (note how the mid-points line up nicely with the dashed line), particularly in the second and fourth transformation (i.e. for <code>log(Income) ~ Education</code> and <code>sqrt(Income) ~ Education</code>).</p>
<p>Note that transforming the data can come at a cost, particularly when interpretation of the results is required. For example, what does a log transformation of Income imply? When dealing with concentrations, it can make theoretical sense to take the log of a value–a good example being the concentration of hydrogen ions in a solution which is usually expressed as a log (pH). It’s always good practice to take a step back from the data transformation workflow and assess where your regression analysis is heading.</p>
</div>
<div id="bootstrapping" class="section level3">
<h3><span class="header-section-number">2.5.2</span> Bootstrapping</h3>
<p>If the assumption of normality of residual is not met, one can overcome this problem by using bootstrapping techniques to come up with regression parameters confidence intervals and <span class="math inline">\(P\)</span>-values. The bootstrap technique involves rerunning an analysis, such as the regression analysis in our case, many times, while randomly resampling from our data each time. In concept, we are acting as though our original <em>sample</em> is the actual population and we are sampling, at random (with replacement), from this <em>pseudo</em> population.</p>
<p>One way to implement a bootstrap is to use the <code>boot</code> package’s function <code>boot()</code>. But before we do, we will need to create our own regression function that will be passed to <code>boot()</code>. This custom function will not only compute a regression model, but it will also return the regression coefficients for each simulation. The following block of code defines our new custom function <code>lm.sim</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm.sim &lt;-<span class="st"> </span>function(formula, data, i)
{
  d.sim &lt;-<span class="st"> </span>data [i,]  
  M.sim &lt;-<span class="st"> </span><span class="kw">lm</span>(formula, <span class="dt">data=</span>d.sim)
  <span class="kw">return</span>(M.sim$coef)
}</code></pre></div>
<p>We can now use the <code>boot()</code> function to run the simulation. Note the call to our custom function <code>lm.sim</code> and the number of bootstrap replicates (i.e. <code>R = 999</code>). Also, don’t forget to load the <code>boot</code> library into the current <code>R</code> session (if not already loaded).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(boot)
M.boot &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">statistic =</span> lm.sim, <span class="dt">formula =</span> Income ~<span class="st"> </span>Education, <span class="dt">data=</span>dat, <span class="dt">R =</span> <span class="dv">999</span>)</code></pre></div>
<p>The results of our bootstrap simulation are now stored in the object <code>M.boot</code>. In essence, a new regression line is created for each simulation. In our working example, we created 999 different regression lines. The following plot shows the first 100 regression lines in light grey. Note how they fluctuate about the original regression line (shown in red). The distribution of these slopes is what is used to compute the confidence interval.</p>
<p><img src="regression_files/figure-html/unnamed-chunk-52-1.png" width="384" /></p>
<p>We can now use the function <code>boot.ci</code> (also in the <code>boot</code> package) to extract confidence intervals for our <span class="math inline">\(\beta\)</span> parameters for a given <span class="math inline">\(\alpha\)</span> confidence value. For example, to get the confidence intervals for the parameters <span class="math inline">\(\hat \beta_0\)</span> at a 95% confidence interval, you can type the following command:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boot.ci</span>(M.boot, <span class="dt">conf =</span> <span class="fl">0.95</span>, <span class="dt">type =</span> <span class="st">&quot;bca&quot;</span>, <span class="dt">index=</span><span class="dv">1</span>)</code></pre></div>
<pre><code>BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 999 bootstrap replicates

CALL : 
boot.ci(boot.out = M.boot, conf = 0.95, type = &quot;bca&quot;, index = 1)

Intervals : 
Level       BCa          
95%   ( 9884, 14096 )  
Calculations and Intervals on Original Scale</code></pre>
<p>Note that your values may differ since the results stem from Monte Carlo simulation. Index <code>1</code>, the first parameter in our lm.sim output, is the coefficient for <span class="math inline">\(\hat \beta_0\)</span>. <code>boot.ci</code> will generate many difference confidence intervals. In this example, we are choosing <code>bca</code> (adjusted bootstrap percentile).</p>
<p>To get the confidence interval for <span class="math inline">\(\hat \beta_1\)</span> at the 95% <span class="math inline">\(\alpha\)</span> level just have the function point to index 2:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boot.ci</span>(M.boot, <span class="dt">conf =</span> <span class="fl">0.95</span>, <span class="dt">type=</span><span class="st">&quot;bca&quot;</span>, <span class="dt">index=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 999 bootstrap replicates

CALL : 
boot.ci(boot.out = M.boot, conf = 0.95, type = &quot;bca&quot;, index = 2)

Intervals : 
Level       BCa          
95%   (42704, 58512 )  
Calculations and Intervals on Original Scale</code></pre>
<p>The following table summarizes the 95% confidence interval for <span class="math inline">\(\hat \beta_1\)</span> from our simulation. The original confidence interval is displayed for comparison.</p>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="center"><span class="math inline">\(\hat \beta_1\)</span> interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Original</td>
<td align="center">[44149, 55192]</td>
</tr>
<tr class="even">
<td align="left">Bootstrap</td>
<td align="center">[42704, 58512]</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="multivariate-regression-model" class="section level1">
<h1><span class="header-section-number">3</span> Multivariate regression model</h1>
<p>We do not need to restrict ourselves to one predictor variable, we can add as many predictor variables to the model as needed (if suggested by theory). But we only do so in the hopes that the extra predictors help account for the unexplained variances in <span class="math inline">\(Y\)</span>.</p>
<p>Continuing with our working example, we will add a second predictor variable, <code>x2</code>. This variable represents the fraction of the employed civilian workforce employed in a professional field (e.g. scientific, management and administrative industries)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x2 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.079</span>, <span class="fl">0.062</span>, <span class="fl">0.116</span>, <span class="fl">0.055</span>, <span class="fl">0.103</span>, <span class="fl">0.078</span>, 
        <span class="fl">0.089</span>, <span class="fl">0.079</span>, <span class="fl">0.067</span>, <span class="fl">0.073</span>, <span class="fl">0.054</span>, <span class="fl">0.094</span>, 
        <span class="fl">0.061</span>, <span class="fl">0.072</span>, <span class="fl">0.038</span>, <span class="fl">0.084</span>)</code></pre></div>
<p>We will add this new variable to our <code>dat</code> dataframe and name the variable <code>Professional</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat$Professional &lt;-<span class="st"> </span>x2</code></pre></div>
<p>We now have an updated dataframe, <code>dat</code>, with a third column.</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Income</th>
<th align="center">Education</th>
<th align="center">Professional</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Androscoggin</td>
<td align="center">23663</td>
<td align="center">0.19</td>
<td align="center">0.079</td>
</tr>
<tr class="even">
<td align="left">Aroostook</td>
<td align="center">20659</td>
<td align="center">0.16</td>
<td align="center">0.062</td>
</tr>
<tr class="odd">
<td align="left">Cumberland</td>
<td align="center">32277</td>
<td align="center">0.40</td>
<td align="center">0.116</td>
</tr>
<tr class="even">
<td align="left">Franklin</td>
<td align="center">21595</td>
<td align="center">0.24</td>
<td align="center">0.055</td>
</tr>
<tr class="odd">
<td align="left">Hancock</td>
<td align="center">27227</td>
<td align="center">0.31</td>
<td align="center">0.103</td>
</tr>
<tr class="even">
<td align="left">Kennebec</td>
<td align="center">25023</td>
<td align="center">0.24</td>
<td align="center">0.078</td>
</tr>
<tr class="odd">
<td align="left">Knox</td>
<td align="center">26504</td>
<td align="center">0.28</td>
<td align="center">0.089</td>
</tr>
<tr class="even">
<td align="left">Lincoln</td>
<td align="center">28741</td>
<td align="center">0.31</td>
<td align="center">0.079</td>
</tr>
<tr class="odd">
<td align="left">Oxford</td>
<td align="center">21735</td>
<td align="center">0.18</td>
<td align="center">0.067</td>
</tr>
<tr class="even">
<td align="left">Penobscot</td>
<td align="center">23366</td>
<td align="center">0.23</td>
<td align="center">0.073</td>
</tr>
<tr class="odd">
<td align="left">Piscataquis</td>
<td align="center">20871</td>
<td align="center">0.17</td>
<td align="center">0.054</td>
</tr>
<tr class="even">
<td align="left">Sagadahoc</td>
<td align="center">28370</td>
<td align="center">0.31</td>
<td align="center">0.094</td>
</tr>
<tr class="odd">
<td align="left">Somerset</td>
<td align="center">21105</td>
<td align="center">0.15</td>
<td align="center">0.061</td>
</tr>
<tr class="even">
<td align="left">Waldo</td>
<td align="center">22706</td>
<td align="center">0.25</td>
<td align="center">0.072</td>
</tr>
<tr class="odd">
<td align="left">Washington</td>
<td align="center">19527</td>
<td align="center">0.19</td>
<td align="center">0.038</td>
</tr>
<tr class="even">
<td align="left">York</td>
<td align="center">28321</td>
<td align="center">0.28</td>
<td align="center">0.084</td>
</tr>
</tbody>
</table>
<p>Let’s rerun our original model, but this time we will call the output of the one predictor model <code>M1</code> (this naming convention will help when comparing models).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Income ~<span class="st"> </span>Education, dat)</code></pre></div>
<p>Let’s review the summary. This time we’ll have <code>R</code> display the parameters <em>and</em> the statistics in a single output:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(M1)</code></pre></div>
<pre><code>
Call:
lm(formula = Income ~ Education, data = dat)

Residuals:
    Min      1Q  Median      3Q     Max 
-2730.4  -490.9   249.5   757.9  2008.8 

Coefficients:
            Estimate Std. Error t value    Pr(&gt;|t|)    
(Intercept)    12404       1350   9.186 0.000000264 ***
Education      49670       5355   9.275 0.000000235 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1432 on 14 degrees of freedom
Multiple R-squared:   0.86, Adjusted R-squared:   0.85 
F-statistic: 86.03 on 1 and 14 DF,  p-value: 0.0000002353</code></pre>
<p>The output should be the same as before.</p>
<p>We now generate an augmented (i.e. adding a second predictor) regression model, <code>M2</code>, as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M2 &lt;-<span class="st"> </span><span class="kw">lm</span>( Income ~<span class="st"> </span>Education +<span class="st"> </span>Professional, dat)</code></pre></div>
<p>At this point, it’s helpful to visualize what a two-variable model represents. A two-variable model defines the equation of a plane that best fits a 3-D set of points. The regression plane is to a two-variable model what a regression line is to a one-variable model. We can plot the 3-D scatter plot where <em>x-axis</em> = fraction with bachelor’s, <em>y-axis</em> = fraction with a professional job and <em>z-axis</em> = income. We draw the 3-D scatter plot using the function <code>scatterplot3d()</code> from the <code>scatterplot3d()</code> package. We also add the regression plane to the plot.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(scatterplot3d) 
s3d &lt;-<span class="st"> </span><span class="kw">scatterplot3d</span>(dat$Education, dat$Professional , dat$Income, 
                     <span class="dt">highlight.3d=</span><span class="ot">TRUE</span>, <span class="dt">angle=</span><span class="dv">55</span>, <span class="dt">scale.y=</span><span class="fl">0.7</span>, <span class="dt">pch=</span><span class="dv">16</span>,
                     <span class="dt">xlab =</span> <span class="st">&quot;Education&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Professional&quot;</span>, <span class="dt">zlab=</span><span class="st">&quot;Income&quot;</span>)

<span class="co"># Add the 3-D regression plane defined by our model M2</span>
s3d$<span class="kw">plane3d</span>(M2, <span class="dt">lty.box=</span><span class="st">&quot;solid&quot;</span>)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<div id="reviewing-the-multivariate-regression-model-summary" class="section level2">
<h2><span class="header-section-number">3.1</span> Reviewing the multivariate regression model summary</h2>
<p>We can, of course, generate a full summary of the regression results as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(M2)</code></pre></div>
<pre><code>
Call:
lm(formula = Income ~ Education + Professional, data = dat)

Residuals:
    Min      1Q  Median      3Q     Max 
-1704.2  -360.6  -202.5   412.8  2006.7 

Coefficients:
             Estimate Std. Error t value    Pr(&gt;|t|)    
(Intercept)     10907       1143   9.544 0.000000308 ***
Education       29684       7452   3.983     0.00156 ** 
Professional    84473      26184   3.226     0.00663 ** 
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1108 on 13 degrees of freedom
Multiple R-squared:  0.9223,    Adjusted R-squared:  0.9103 
F-statistic: 77.13 on 2 and 13 DF,  p-value: 0.00000006148</code></pre>
<p>The interpretation of the model statistics is the same with a multivariate model as it is with a bivariate model. The one difference is that the extra regression coefficient <span class="math inline">\(\hat \beta_2\)</span> (associated with the <em>Professional</em> variable) is added to the list of regression parameters. In our example, <span class="math inline">\(\hat \beta_2\)</span> is significantly different from <span class="math inline">\(0\)</span> (<span class="math inline">\(P\)</span> = 0.0066).</p>
<p>One noticeable difference in the summary output is the presence of a <strong>Multiple R-squared</strong> statistic in lieu of the bivariate simple R-squared. Its interpretation is, however, the same and we can note an increase in the models’ overall <span class="math inline">\(R^2\)</span> value. The <span class="math inline">\(F\)</span>-statistic’s <span class="math inline">\(P\)</span> value of <span class="math inline">\(0\)</span> tells us that the amount of residual error from this multivariate model is significantly less than that of the simpler model, the mean <span class="math inline">\(\bar Y\)</span>. So far, things look encouraging.</p>
<p>The equation of the modeled plane is thus:</p>
<p><span class="math inline">\(\widehat{Income}\)</span> = <strong>10907</strong> + <strong>29684</strong> (<span class="math inline">\(Education\)</span>) + <strong>84473</strong> (<span class="math inline">\(Professional\)</span>)</p>
</div>
<div id="comparing-the-two-variable-model-with-the-one-variable-model" class="section level2">
<h2><span class="header-section-number">3.2</span> Comparing the two-variable model with the one-variable model</h2>
<p>We can compare the two-variable model, <code>M2</code>, with the one-variable model (the bivariate model), <code>M1</code>. Using the <code>anova()</code> function. This test assesses whether or not adding the second variable, <code>x2</code>, <em>significantly</em> improves our ability to predict <span class="math inline">\(Y\)</span>. It’s important to remember that a good model is a parsimonious one. If an augmented regression model does not significantly improve our overall predictive ability, then we should always revert back to the simpler model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(M1, M2)</code></pre></div>
<pre><code>Analysis of Variance Table

Model 1: Income ~ Education
Model 2: Income ~ Education + Professional
  Res.Df      RSS Df Sum of Sq      F   Pr(&gt;F)   
1     14 28724435                                
2     13 15952383  1  12772052 10.408 0.006625 **
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <span class="math inline">\(P\)</span> value is very small indicating that the reduction in residual error in model <code>M2</code> over model <code>M1</code> is significant. Model <code>M2</code> looks promising so far.</p>
</div>
<div id="looking-for-multi-collinearity" class="section level2">
<h2><span class="header-section-number">3.3</span> Looking for multi-collinearity</h2>
<p>When adding explanatory variables to a model, one must be very careful not to add variables that explain the same ‘thing’. In other words, we don’t want (nor need) to have two or more variables in the model explain the <strong>same variance</strong>. One popular test for Multi-collinearity is the <strong>Variance Inflation Factor (VIF)</strong> test. The VIF is computed for all regression parameters. For example, VIF for <span class="math inline">\(X_2\)</span> is computed by first regressing <span class="math inline">\(X_1\)</span> against all other predictors (in our example, we would regress <span class="math inline">\(X_1\)</span> against <span class="math inline">\(X_2\)</span>) then taking the resulting <span class="math inline">\(R^2\)</span> and plugging it in the following VIF equation: <span class="math display">\[
VIF = \frac{1}{1 - R^2_1}
\]</span></p>
<p>where the subscript <span class="math inline">\(1\)</span> in <span class="math inline">\(R^2_1\)</span> indicates that the <span class="math inline">\(R^2\)</span> is for the model <code>x1 ~ x2</code>. The VIF for <span class="math inline">\(X_2\)</span> is computed in the same way.</p>
<p>What we are avoiding are <em>large</em> VIF values (a <em>large</em> VIF value may indicate high multicollinearity). What constitutes a large VIF value is open for debate. Typical values range from 3 to 10, so VIF’s should be interpreted with caution.</p>
<p>We can use <code>R</code>’s <code>vif()</code> function to compute VIF for the variables <em>Education</em> and <em>Professional</em></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(M2)</code></pre></div>
<pre><code>   Education Professional 
    3.237621     3.237621 </code></pre>
<p>The <em>moderately high</em> VIF should be a warning. It’s always good practice to generate a <strong>scatter plot matrix</strong> to <em>view</em> any potential relationship between <em>all</em> variables involved. We can use <code>R</code>’s <code>pairs()</code> function to display the scatter plots:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pairs</span>(dat, <span class="dt">panel =</span> panel.smooth)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-66-1.png" width="576" /></p>
<p>The function <code>pairs()</code> generates a scatter plot matrix of all columns in the dataframe. Three columns of data are present in our dataframe <code>dat</code>, therefore we have a 3x3 scatter plot matrix. We also add a LOWESS smoother (shown as a red polyline in each graph) by calling the option <code>panel = panel.smooth</code>. To interpret the output, just match each plot to its corresponding variable along the <em>x-axis</em> and <em>y-axis</em>. For example, the scatter plot in the upper right-hand corner is that for the variables <em>Professional</em> and <em>Income</em>. We are interested in seeing how the predictor variables <em>Education</em> and <em>Professional</em> relate to one another. The scatter plot between both predictor variables seem to indicate a significant relationship between the two predictors–this does not bode well for our two-predictor model. The fact that they seem to be correlated suggests that they may be explaining the same variance in <span class="math inline">\(Y\)</span>. This means that despite having <em>promising</em> model summary statistics, we cannot trust this model and thus revert back to our bivariate model <code>M1</code>.</p>
</div>
</div>
<div id="including-categorical-predictor-variables" class="section level1">
<h1><span class="header-section-number">4</span> Including categorical predictor variables</h1>
<div id="two-category-variable" class="section level2">
<h2><span class="header-section-number">4.1</span> Two category variable</h2>
<p>So far, we have worked with continuous variables. We can include categorical variables in our model as well. For example, let’s hypothesize that coastal counties might be conducive to higher incomes. We’ll create a new variable, <code>x3</code>, that will house one of two values, <code>yes</code> or <code>no</code>, indicating whether or not the county is on the coast or not.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x3 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;yes&quot;</span>,
        <span class="st">&quot;no&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;yes&quot;</span>)</code></pre></div>
<p>Alternatively, we could have encoded <em>access to coast</em> as binary values <code>0</code> and <code>1</code>, but as we’ll see shortly, when we have non-numeric variables in a regression model, <code>R</code> will recognize such variables as <strong>factors</strong>.</p>
<p>We will add these values to our <code>dat</code> dataframe and name the new variable <code>Coast</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat$Coast &lt;-<span class="st"> </span>x3</code></pre></div>
<p>The way a categorical variable such as <code>Coast</code> is treated in a regression model is by converting categories to binary values. In our example, the variable <code>Coast</code> has two categories, <code>yes</code> and <code>no</code>, which means that one will be coded <code>0</code> and the other <code>1</code>. You could specify which is to be coded <code>0</code> or <code>1</code>, or you can let <code>R</code> do this for you. Let’s run the regression analysis with our new variable (we will not use the variable <em>Professional</em> since we concluded earlier that the variable was redundant).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M3 &lt;-<span class="st"> </span><span class="kw">lm</span>( Income ~<span class="st"> </span>Education +<span class="st"> </span>Coast, <span class="dt">dat =</span> dat)</code></pre></div>
<p>You may see a warning message indicating that the variable Coast was converted to a factor (i.e. it was treated as a categorical value). Conversely, if you want to explicitly tell <code>R</code> that the variable Coast should be treated as a category, you can enclose that variable in the <code>as.factor()</code> function as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M3 &lt;-<span class="st"> </span><span class="kw">lm</span>( Income ~<span class="st"> </span>Education +<span class="st"> </span><span class="kw">as.factor</span>(Coast), <span class="dt">dat =</span> dat)</code></pre></div>
<p>The <code>as.factor</code> option is important if your categorical variable is numeric since <code>R</code> would treat such variable as numeric.</p>
<p>We can view the full summary as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(M3)</code></pre></div>
<pre><code>
Call:
lm(formula = Income ~ Education + as.factor(Coast), data = dat)

Residuals:
    Min      1Q  Median      3Q     Max 
-3054.9  -548.7   277.0   754.5  2211.3 

Coefficients:
                    Estimate Std. Error t value   Pr(&gt;|t|)    
(Intercept)          11861.6     1621.8   7.314 0.00000588 ***
Education            53284.9     7882.2   6.760 0.00001341 ***
as.factor(Coast)yes   -671.7     1054.1  -0.637      0.535    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1464 on 13 degrees of freedom
Multiple R-squared:  0.8643,    Adjusted R-squared:  0.8434 
F-statistic: 41.39 on 2 and 13 DF,  p-value: 0.000002303</code></pre>
<p>You’ll note that the regression coefficient for <em>Coast</em> is displayed as the variable <code>Coastyes</code>, in other words <code>R</code> chose one of the categories, <code>yes</code> to be coded as <code>1</code> for us. The regression model is thus:</p>
<p><span class="math inline">\(\widehat{Income}\)</span> = <strong>11862</strong> + <strong>53285</strong> <span class="math inline">\(Education\)</span> <strong>-672</strong> <span class="math inline">\(Coast_{yes}\)</span></p>
<p>The last variable is treated as follows: if the county is on the coast, then <code>Coast</code> takes on a value of <strong>1</strong>, if not, it takes on a value of <strong>0</strong>. For example, <strong>Androscoggin</strong> county which is <em>not</em> on the coast is expressed as follows where the variable <em>Coast</em> takes on the value of <strong>0</strong>:</p>
<p><span class="math inline">\(\widehat{Income}\)</span> = <strong>11862</strong> + <strong>53285</strong> (23663) <strong>-672</strong> (0)</p>
<p>The equation for <strong>Cumberland</strong> county which <em>is</em> on the coast is expressed as follows where the variable <em>Coast</em> takes on the value of <strong>1</strong>:</p>
<p><span class="math inline">\(\widehat{Income}\)</span> = <strong>11862</strong> + <strong>53285</strong> (32277) <strong>-672</strong> (1)</p>
<p>Turning our attention to the model summary output, one notices a relatively high <span class="math inline">\(P\)</span>-value associated with the <em>Coast</em> regression coefficient. This indicates that <span class="math inline">\(\beta_2\)</span> is not significantly different from <span class="math inline">\(0\)</span>. In fact, comparing this model with model <code>M1</code> using <code>anova()</code> indicates that adding the <em>Coast</em> variable does not significantly improve our prediction of <span class="math inline">\(Y\)</span> (income).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(M1, M3)</code></pre></div>
<pre><code>Analysis of Variance Table

Model 1: Income ~ Education
Model 2: Income ~ Education + as.factor(Coast)
  Res.Df      RSS Df Sum of Sq     F Pr(&gt;F)
1     14 28724435                          
2     13 27854540  1    869895 0.406 0.5351</code></pre>
<p>Note the large <span class="math inline">\(P\)</span> value of 0.54 indicating that <span class="math inline">\(\beta_{coast}\)</span> does not improve the prediction of <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="multi-category-variable" class="section level2">
<h2><span class="header-section-number">4.2</span> Multi-category variable</h2>
<p>For each category in a categorical variable, there are <span class="math inline">\((number\; of\; categories\; -1)\)</span> regression variables added to the model. In the previous example, the variable <em>Coast</em> had two categories, <em>yes</em> and <em>no</em>, therefore <strong>one</strong> regression parameter was added to the model. If the categorical variable has three categories, then two regression parameters are added to the model. For example, let’s split the state of Maine into three zones–east, central and west–we end up with the following explanatory variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x4 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;W&quot;</span>, <span class="st">&quot;C&quot;</span>, <span class="st">&quot;W&quot;</span>, <span class="st">&quot;W&quot;</span>, <span class="st">&quot;E&quot;</span>, <span class="st">&quot;C&quot;</span>, <span class="st">&quot;C&quot;</span>, <span class="st">&quot;C&quot;</span>, <span class="st">&quot;W&quot;</span>, <span class="st">&quot;C&quot;</span>,
        <span class="st">&quot;C&quot;</span>, <span class="st">&quot;C&quot;</span>, <span class="st">&quot;W&quot;</span>, <span class="st">&quot;E&quot;</span>, <span class="st">&quot;E&quot;</span>, <span class="st">&quot;W&quot;</span>)</code></pre></div>
<p>We’ll add this fourth predictor variable we’ll call <code>Geo</code> to our dataset <code>dat</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat$Geo &lt;-<span class="st"> </span>x4</code></pre></div>
<p>Our table now looks like this:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Income</th>
<th align="right">Education</th>
<th align="right">Professional</th>
<th align="left">Coast</th>
<th align="left">Geo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Androscoggin</td>
<td align="right">23663</td>
<td align="right">0.19</td>
<td align="right">0.079</td>
<td align="left">no</td>
<td align="left">W</td>
</tr>
<tr class="even">
<td align="left">Aroostook</td>
<td align="right">20659</td>
<td align="right">0.16</td>
<td align="right">0.062</td>
<td align="left">no</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="left">Cumberland</td>
<td align="right">32277</td>
<td align="right">0.40</td>
<td align="right">0.116</td>
<td align="left">yes</td>
<td align="left">W</td>
</tr>
<tr class="even">
<td align="left">Franklin</td>
<td align="right">21595</td>
<td align="right">0.24</td>
<td align="right">0.055</td>
<td align="left">no</td>
<td align="left">W</td>
</tr>
<tr class="odd">
<td align="left">Hancock</td>
<td align="right">27227</td>
<td align="right">0.31</td>
<td align="right">0.103</td>
<td align="left">yes</td>
<td align="left">E</td>
</tr>
<tr class="even">
<td align="left">Kennebec</td>
<td align="right">25023</td>
<td align="right">0.24</td>
<td align="right">0.078</td>
<td align="left">no</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="left">Knox</td>
<td align="right">26504</td>
<td align="right">0.28</td>
<td align="right">0.089</td>
<td align="left">yes</td>
<td align="left">C</td>
</tr>
<tr class="even">
<td align="left">Lincoln</td>
<td align="right">28741</td>
<td align="right">0.31</td>
<td align="right">0.079</td>
<td align="left">yes</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="left">Oxford</td>
<td align="right">21735</td>
<td align="right">0.18</td>
<td align="right">0.067</td>
<td align="left">no</td>
<td align="left">W</td>
</tr>
<tr class="even">
<td align="left">Penobscot</td>
<td align="right">23366</td>
<td align="right">0.23</td>
<td align="right">0.073</td>
<td align="left">no</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="left">Piscataquis</td>
<td align="right">20871</td>
<td align="right">0.17</td>
<td align="right">0.054</td>
<td align="left">no</td>
<td align="left">C</td>
</tr>
<tr class="even">
<td align="left">Sagadahoc</td>
<td align="right">28370</td>
<td align="right">0.31</td>
<td align="right">0.094</td>
<td align="left">yes</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="left">Somerset</td>
<td align="right">21105</td>
<td align="right">0.15</td>
<td align="right">0.061</td>
<td align="left">no</td>
<td align="left">W</td>
</tr>
<tr class="even">
<td align="left">Waldo</td>
<td align="right">22706</td>
<td align="right">0.25</td>
<td align="right">0.072</td>
<td align="left">yes</td>
<td align="left">E</td>
</tr>
<tr class="odd">
<td align="left">Washington</td>
<td align="right">19527</td>
<td align="right">0.19</td>
<td align="right">0.038</td>
<td align="left">yes</td>
<td align="left">E</td>
</tr>
<tr class="even">
<td align="left">York</td>
<td align="right">28321</td>
<td align="right">0.28</td>
<td align="right">0.084</td>
<td align="left">yes</td>
<td align="left">W</td>
</tr>
</tbody>
</table>
<p>Now let’s run a fourth regression analysis using the variables <code>Education</code> and <code>Geo</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M4 &lt;-<span class="st"> </span><span class="kw">lm</span>(Income ~<span class="st"> </span>Education +<span class="st"> </span>Geo, <span class="dt">dat =</span> dat)</code></pre></div>
<p>Again, you might see a warning indicating that the variable <code>Geo</code> was flagged as a categorical variable and thus converted to a <code>factor</code>.</p>
<p>Let’s look at the model summary.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(M4)</code></pre></div>
<pre><code>
Call:
lm(formula = Income ~ Education + Geo, data = dat)

Residuals:
    Min      1Q  Median      3Q     Max 
-3187.7  -473.2     1.9   642.7  1527.1 

Coefficients:
            Estimate Std. Error t value    Pr(&gt;|t|)    
(Intercept)  12579.3     1217.9  10.329 0.000000252 ***
Education    50281.5     4630.6  10.858 0.000000146 ***
GeoE         -1996.4      854.1  -2.337      0.0376 *  
GeoW           135.8      688.2   0.197      0.8469    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1237 on 12 degrees of freedom
Multiple R-squared:  0.9106,    Adjusted R-squared:  0.8882 
F-statistic: 40.72 on 3 and 12 DF,  p-value: 0.000001443</code></pre>
<p>As expected, the model added <strong>two</strong> regression parameters, one is called <code>GeoE</code> for counties identified as being on the east side of the state and <code>GeoW</code> for the counties identified as being on the west side of the state. Here are three examples showing how the equation is to be interpreted with different east/central/west designations.</p>
<p>For <strong>Androscoggin</strong> county, its location lies to the west, therefore the variable <code>GeoE</code> is assigned a value of 0 and <code>GeoW</code> is assigned a value of <code>1</code>:</p>
<p><span class="math inline">\(\widehat{Income}\)</span> = <strong>12579</strong> + <strong>50282</strong> (23663) <strong>-1996</strong> (0) + <strong>136</strong> (1)</p>
<p>For <strong>Hancock</strong> county, its location lies to the east, therefore the variable <code>GeoE</code> is assigned a value of <strong>1</strong> and <code>GeoW</code> is assigned a value of <strong>0</strong>:</p>
<p><span class="math inline">\(\widehat{Income}\)</span> = <strong>12579</strong> + <strong>50282</strong> (27227) <strong>-1996</strong> (1) + <strong>136</strong> (0)</p>
<p>For <strong>Kennebec</strong> county, its location lies in the center of the state, therefore <strong>both</strong> variables <code>GeoE</code> and <code>GeoW</code> are assigned a value of <strong>0</strong>:</p>
<p><span class="math inline">\(\widehat{Income}\)</span> = <strong>12579</strong> + <strong>50282</strong> (25023) <strong>-1996</strong> (0) + <strong>136</strong> (0)</p>
<p>Turning to the model summary results, it appears that one of the new regression coefficients, <span class="math inline">\(\beta_{GeoE}\)</span> is significantly different from 0 whereas that of <span class="math inline">\(\beta_{GeoW}\)</span> is not. This seems to suggest that Eastern counties might differ from other counties when it comes to per-capita income. We can aggregate the western and central counties into a single category and end up with a two category variable, i.e one where a county is either on the eastern side of the state or is not. We will use the <code>recode()</code> function to reclassify the values and add the new codes to our <code>dat</code> dataframe as a new variable we’ll call <code>East</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat$East &lt;-<span class="st"> </span><span class="kw">recode</span>(dat$Geo, <span class="st">&quot; &#39;W&#39; = &#39;No&#39; ; &#39;C&#39; = &#39;No&#39; ; &#39;E&#39; = &#39;Yes&#39; &quot;</span>)</code></pre></div>
<p>Let’s look at our augmented table.</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Income</th>
<th align="right">Education</th>
<th align="right">Professional</th>
<th align="left">Coast</th>
<th align="left">Geo</th>
<th align="left">East</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Androscoggin</td>
<td align="right">23663</td>
<td align="right">0.19</td>
<td align="right">0.079</td>
<td align="left">no</td>
<td align="left">W</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Aroostook</td>
<td align="right">20659</td>
<td align="right">0.16</td>
<td align="right">0.062</td>
<td align="left">no</td>
<td align="left">C</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Cumberland</td>
<td align="right">32277</td>
<td align="right">0.40</td>
<td align="right">0.116</td>
<td align="left">yes</td>
<td align="left">W</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Franklin</td>
<td align="right">21595</td>
<td align="right">0.24</td>
<td align="right">0.055</td>
<td align="left">no</td>
<td align="left">W</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Hancock</td>
<td align="right">27227</td>
<td align="right">0.31</td>
<td align="right">0.103</td>
<td align="left">yes</td>
<td align="left">E</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">Kennebec</td>
<td align="right">25023</td>
<td align="right">0.24</td>
<td align="right">0.078</td>
<td align="left">no</td>
<td align="left">C</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Knox</td>
<td align="right">26504</td>
<td align="right">0.28</td>
<td align="right">0.089</td>
<td align="left">yes</td>
<td align="left">C</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Lincoln</td>
<td align="right">28741</td>
<td align="right">0.31</td>
<td align="right">0.079</td>
<td align="left">yes</td>
<td align="left">C</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Oxford</td>
<td align="right">21735</td>
<td align="right">0.18</td>
<td align="right">0.067</td>
<td align="left">no</td>
<td align="left">W</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Penobscot</td>
<td align="right">23366</td>
<td align="right">0.23</td>
<td align="right">0.073</td>
<td align="left">no</td>
<td align="left">C</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Piscataquis</td>
<td align="right">20871</td>
<td align="right">0.17</td>
<td align="right">0.054</td>
<td align="left">no</td>
<td align="left">C</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Sagadahoc</td>
<td align="right">28370</td>
<td align="right">0.31</td>
<td align="right">0.094</td>
<td align="left">yes</td>
<td align="left">C</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">Somerset</td>
<td align="right">21105</td>
<td align="right">0.15</td>
<td align="right">0.061</td>
<td align="left">no</td>
<td align="left">W</td>
<td align="left">No</td>
</tr>
<tr class="even">
<td align="left">Waldo</td>
<td align="right">22706</td>
<td align="right">0.25</td>
<td align="right">0.072</td>
<td align="left">yes</td>
<td align="left">E</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">Washington</td>
<td align="right">19527</td>
<td align="right">0.19</td>
<td align="right">0.038</td>
<td align="left">yes</td>
<td align="left">E</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">York</td>
<td align="right">28321</td>
<td align="right">0.28</td>
<td align="right">0.084</td>
<td align="left">yes</td>
<td align="left">W</td>
<td align="left">No</td>
</tr>
</tbody>
</table>
<p>At this point, we could run a new regression with this variable, but before we do, we will control which category from the variable <code>East</code> will be assigned the <strong>reference</strong> category in the regression model as opposed to letting <code>R</code> decide for us. To complete this task, we will use the function <code>factor()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat$East &lt;-<span class="st"> </span><span class="kw">factor</span>(dat$East, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;No&quot;</span>, <span class="st">&quot;Yes&quot;</span>))</code></pre></div>
<p>This line of code does three things: it explicitly defines the variable <code>East</code> as a factor, it explicitly defines the two categories, <code>No</code> and <code>Yes</code>, and it sets the order of these categories (based on the order of the categories defined in the <code>level</code> option). The latter is important since the regression function <code>lm</code> will assign the first category in the level as the reference.</p>
<p>We are now ready to run the regression analysis</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">M5 &lt;-<span class="st"> </span><span class="kw">lm</span>(Income ~<span class="st"> </span>Education +<span class="st"> </span>East, dat)</code></pre></div>
<p>Let’s view the regression model summary.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(M5)</code></pre></div>
<pre><code>
Call:
lm(formula = Income ~ Education + East, data = dat)

Residuals:
     Min       1Q   Median       3Q      Max 
-3114.59  -454.21     5.88   614.49  1600.85 

Coefficients:
            Estimate Std. Error t value     Pr(&gt;|t|)    
(Intercept)  12646.2     1125.6  11.235 0.0000000459 ***
Education    50264.0     4455.3  11.282 0.0000000437 ***
EastYes      -2058.9      763.3  -2.697       0.0183 *  
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 1190 on 13 degrees of freedom
Multiple R-squared:  0.9103,    Adjusted R-squared:  0.8965 
F-statistic: 65.93 on 2 and 13 DF,  p-value: 0.0000001564</code></pre>
<p>This model is an improvement over the model <code>M1</code> in that a bit more of the variance in <span class="math inline">\(Y\)</span> is accounted for in <code>M5</code> as gleaned from the higher <span class="math inline">\(R^2\)</span> value. We also note that <strong>all</strong> regression coefficients are significantly different from 0. This is a good thing. The negative regression coefficient <span class="math inline">\(\hat {\beta_2}\)</span> tells us that income is negatively related to the East-other designation. In other words, income tends to be less in eastern counties than in other counties.</p>
<p>Now let’s look at the <code>lm</code> summary plots to see if the assumptions pertaining to the residuals are met:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">OP &lt;-<span class="st"> </span><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(M5, <span class="dt">which =</span> <span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">4</span>))
<span class="kw">par</span>(OP)</code></pre></div>
<p><img src="regression_files/figure-html/unnamed-chunk-83-1.png" width="720" /></p>
<p>You may recall back when we looked at the residuals for model <code>M1</code> that the Q-Q plot showed residuals near the tails of the distribution that diverged from the theoretical distribution. It appears that this divergence near the ends of the distribution is under control–that’s a good thing. There is also no strong evidence of heteroscadisticy and the Cooks’ Distance indicates that there is no disproportionately influential points (all values are less than 1).</p>
<p>Bear in mind that the geographic designation of East vs. Others may have more to do with factors associated with the two eastern most counties than actual geographic location. It may well be that our variable <code>East</code> is nothing more than a latent variable, but one that may shed some valuable insight into the prediction of <span class="math inline">\(Y\)</span>.</p>
</div>
</div>
<div id="references" class="section level1">
<h1><span class="header-section-number">5</span> References</h1>
<p>Freedman D.A., Robert Pisani, Roger Purves. <em>Statistics</em>, 4th edition, 2007.<br />
Millard S.P, Neerchal N.K., <em>Environmental Statistics with S-Plus</em>, 2001.<br />
McClave J.T., Dietrich F.H., <em>Statistics</em>, 4th edition, 1988.<br />
Vik P., <em>Regression, ANOVA, and the General Linear Model: A Statistics Primer</em>, 2013.</p>
<p><strong>Session Info</strong>:</p>
<p><strong>R version 3.3.0 (2016-05-03)</strong></p>
<p>**<a href="Platform:**" class="uri">Platform:**</a> x86_64-w64-mingw32/x64 (64-bit)</p>
<p><strong>attached base packages:</strong> <em>stats</em>, <em>graphics</em>, <em>grDevices</em>, <em>utils</em>, <em>datasets</em>, <em>methods</em> and <em>base</em></p>
<p><strong>other attached packages:</strong> <em>scatterplot3d(v.0.3-36)</em>, <em>car(v.2.1-2)</em>, <em>ggplot2(v.2.1.0)</em>, <em>boot(v.1.3-18)</em>, <em>gplots(v.3.0.1)</em>, <em>MASS(v.7.3-45)</em> and <em>tidyr(v.0.4.1)</em></p>
<p><strong>loaded via a namespace (and not attached):</strong> <em>gtools(v.3.5.0)</em>, <em>pander(v.0.6.0)</em>, <em>splines(v.3.3.0)</em>, <em>lattice(v.0.20-33)</em>, <em>colorspace(v.1.2-6)</em>, <em>miniUI(v.0.1.1)</em>, <em>htmltools(v.0.3.5)</em>, <em>yaml(v.2.1.13)</em>, <em>mgcv(v.1.8-12)</em>, <em>nloptr(v.1.0.4)</em>, <em>DBI(v.0.4-1)</em>, <em>plyr(v.1.8.3)</em>, <em>stringr(v.1.0.0)</em>, <em>MatrixModels(v.0.4-1)</em>, <em>munsell(v.0.4.3)</em>, <em>gtable(v.0.2.0)</em>, <em>caTools(v.1.17.1)</em>, <em>evaluate(v.0.9)</em>, <em>labeling(v.0.3)</em>, <em>knitr(v.1.13.1)</em>, <em>SparseM(v.1.7)</em>, <em>extrafont(v.0.17)</em>, <em>httpuv(v.1.3.3)</em>, <em>quantreg(v.5.24)</em>, <em>pbkrtest(v.0.4-6)</em>, <em>parallel(v.3.3.0)</em>, <em>Rttf2pt1(v.1.3.4)</em>, <em>highr(v.0.6)</em>, <em>Rcpp(v.0.12.5)</em>, <em>KernSmooth(v.2.23-15)</em>, <em>xtable(v.1.8-2)</em>, <em>scales(v.0.4.0)</em>, <em>formatR(v.1.4)</em>, <em>gdata(v.2.17.0)</em>, <em>mime(v.0.4)</em>, <em>lme4(v.1.1-12)</em>, <em>digest(v.0.6.9)</em>, <em>stringi(v.1.0-1)</em>, <em>bookdown(v.0.0.71)</em>, <em>dplyr(v.0.4.3)</em>, <em>shiny(v.0.13.2)</em>, <em>grid(v.3.3.0)</em>, <em>tools(v.3.3.0)</em>, <em>bitops(v.1.0-6)</em>, <em>magrittr(v.1.5)</em>, <em>lazyeval(v.0.1.10)</em>, <em>extrafontdb(v.1.0)</em>, <em>Matrix(v.1.2-6)</em>, <em>assertthat(v.0.1)</em>, <em>minqa(v.1.2.4)</em>, <em>rmarkdown(v.0.9.6.9)</em>, <em>R6(v.2.1.2)</em>, <em>nnet(v.7.3-12)</em> and <em>nlme(v.3.1-127)</em></p>
</div>


<div class="footer">
<hr/>
<a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/80x15.png" /></a>  Manny Gimond 
</br>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
